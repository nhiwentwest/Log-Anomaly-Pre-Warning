{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸŽ¯ Multi-Layer Log Anomaly Detection - Complete Pipeline\n",
                "\n",
                "**Chá»‰ 5 cells:**\n",
                "1. Setup + Load ALL data\n",
                "2. N-gram + Graph methods\n",
                "3. LogBERT training\n",
                "4. Ensemble + Results\n",
                "5. SimCLR + BGL (optional)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Device: cuda\n",
                        "Data: /teamspace/studios/this_studio/content/LogHub_HDFS\n",
                        "\n",
                        "============================================================\n",
                        "LOADING V1 DATA\n",
                        "============================================================\n",
                        "V1: 575,061 rows, Labels: ['Success' 'Fail']\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "V1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 575061/575061 [00:31<00:00, 17970.80it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "  Normal: 558,223, Failure: 16,838\n",
                        "\n",
                        "============================================================\n",
                        "LOADING V3 DATA\n",
                        "============================================================\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "normal_trace.csv: 46it [1:59:08, 155.41s/it]\n",
                        "failure_trace.csv: 6it [15:24, 154.16s/it]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "  Normal: 226,767, Failure: 29,817, Templates: 2155\n",
                        "\n",
                        "============================================================\n",
                        "PARSING V2 LOGS (may take 1-2 hours for 12GB)\n",
                        "============================================================\n",
                        "Found 31 log files\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Parsing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 31/31 [25:12<00:00, 48.79s/it] \n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "  Sequences: 2,460,745, Templates: 38810\n",
                        "\n",
                        "============================================================\n",
                        "COMBINING DATA\n",
                        "============================================================\n",
                        "Combined Normal: 3,245,735\n",
                        "  V2: 2,460,745, V1: 558,223, V3: 226,767\n",
                        "Combined Failure: 46,655\n",
                        "Pre-fail windows: 46,654\n",
                        "Vocab size: 38735\n"
                    ]
                }
            ],
            "source": [
                "#=============================================================================\n",
                "# CELL 1: SETUP + LOAD ALL DATA (V1 + V2 + V3)\n",
                "#=============================================================================\n",
                "import os, gc, json, random, re, math\n",
                "from datetime import datetime\n",
                "from collections import defaultdict, Counter\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "from tqdm import tqdm\n",
                "\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "from sklearn.metrics import precision_recall_curve, roc_auc_score\n",
                "\n",
                "!pip install networkx drain3 -q\n",
                "import networkx as nx\n",
                "from drain3 import TemplateMiner\n",
                "from drain3.template_miner_config import TemplateMinerConfig\n",
                "\n",
                "# === CONFIG ===\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f\"Device: {device}\")\n",
                "\n",
                "POSSIBLE_PATHS = [\n",
                "    \"/teamspace/studios/this_studio/content/LogHub_HDFS\",\n",
                "    \"/teamspace/studios/this_studio/LogHub_HDFS\",\n",
                "    \"/content/LogHub_HDFS\",\n",
                "    \"./LogHub_HDFS\",\n",
                "]\n",
                "BASE_PATH = next((p for p in POSSIBLE_PATHS if os.path.exists(p)), None)\n",
                "if not BASE_PATH: raise FileNotFoundError(\"LogHub_HDFS not found!\")\n",
                "print(f\"Data: {BASE_PATH}\")\n",
                "\n",
                "V1_PATH = os.path.join(BASE_PATH, \"HDFS_v1/preprocessed/Event_traces.csv\")\n",
                "V2_PATH = os.path.join(BASE_PATH, \"HDFS_v2/node_logs\")\n",
                "V3_NORMAL = os.path.join(BASE_PATH, \"HDFS_v3_TraceBench/preprocessed/normal_trace.csv\")\n",
                "V3_FAILURE = os.path.join(BASE_PATH, \"HDFS_v3_TraceBench/preprocessed/failure_trace.csv\")\n",
                "OUTPUT_DIR = \"output\"\n",
                "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
                "\n",
                "CONTEXT_LEN, D_MODEL, N_HEADS, N_LAYERS = 128, 256, 8, 4\n",
                "BATCH_SIZE, EPOCHS, LR = 64, 20, 1e-4\n",
                "MASK_RATIO, LAMBDA_VHM, PREFAIL_WINDOW = 0.15, 0.5, 10\n",
                "SEED = 42\n",
                "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
                "\n",
                "# === LOAD V1 (cÃ³ labels + real sequences) ===\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"LOADING V1 DATA\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "v1_normal, v1_failure = [], []\n",
                "if os.path.exists(V1_PATH):\n",
                "    df = pd.read_csv(V1_PATH)\n",
                "    print(f\"V1: {len(df):,} rows, Labels: {df['Label'].unique()}\")\n",
                "    \n",
                "    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"V1\"):\n",
                "        features = str(row.get('Features', '[]')).strip('[]\"')\n",
                "        events = [int(e.strip().strip(\"'\")[1:]) for e in features.split(',') \n",
                "                  if e.strip().startswith('E')]\n",
                "        if events:\n",
                "            if row['Label'] == 'Success':\n",
                "                v1_normal.append(events)\n",
                "            else:\n",
                "                v1_failure.append(events)\n",
                "    print(f\"  Normal: {len(v1_normal):,}, Failure: {len(v1_failure):,}\")\n",
                "else:\n",
                "    print(\"V1 not found\")\n",
                "\n",
                "# === LOAD V3 (one-hot preprocessed) ===\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"LOADING V3 DATA\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "def load_v3(filepath):\n",
                "    header = pd.read_csv(filepath, nrows=0)\n",
                "    cols = [c for c in header.columns if c != 'TaskID']\n",
                "    seqs = []\n",
                "    for chunk in tqdm(pd.read_csv(filepath, chunksize=5000), desc=os.path.basename(filepath)):\n",
                "        for i in range(len(chunk)):\n",
                "            seqs.append(np.where(chunk[cols].values[i] == 1)[0].tolist())\n",
                "        gc.collect()\n",
                "    return seqs, len(cols)\n",
                "\n",
                "v3_normal, n_templates = load_v3(V3_NORMAL)\n",
                "v3_failure, _ = load_v3(V3_FAILURE)\n",
                "print(f\"  Normal: {len(v3_normal):,}, Failure: {len(v3_failure):,}, Templates: {n_templates}\")\n",
                "\n",
                "# === PARSE V2 (raw logs with Drain) ===\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"PARSING V2 LOGS (may take 1-2 hours for 12GB)\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "v2_seqs = []\n",
                "v2_templates = 0\n",
                "\n",
                "if os.path.exists(V2_PATH):\n",
                "    log_files = sorted([f for f in os.listdir(V2_PATH) if f.endswith('.log')])\n",
                "    print(f\"Found {len(log_files)} log files\")\n",
                "    \n",
                "    config = TemplateMinerConfig()\n",
                "    config.drain_depth, config.drain_sim_th = 4, 0.4\n",
                "    config.profiling_enabled = False\n",
                "    miner = TemplateMiner(config=config)\n",
                "    \n",
                "    block_events = defaultdict(list)\n",
                "    block_re = re.compile(r'blk_-?\\d+')\n",
                "    ts_re = re.compile(r'(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2})')\n",
                "    \n",
                "    for log_file in tqdm(log_files, desc=\"Parsing\"):\n",
                "        try:\n",
                "            with open(os.path.join(V2_PATH, log_file), 'r', errors='ignore') as f:\n",
                "                for line in f:\n",
                "                    line = line.strip()\n",
                "                    if not line: continue\n",
                "                    blk = block_re.search(line)\n",
                "                    ts = ts_re.search(line)\n",
                "                    result = miner.add_log_message(line)\n",
                "                    if blk:\n",
                "                        block_events[blk.group()].append((ts.group() if ts else None, result['cluster_id']))\n",
                "        except Exception as e:\n",
                "            print(f\"Error {log_file}: {e}\")\n",
                "        gc.collect()\n",
                "    \n",
                "    # Build sequences with temporal order\n",
                "    for blk, events in block_events.items():\n",
                "        events.sort(key=lambda x: x[0] or '')\n",
                "        seq = [tid for _, tid in events]\n",
                "        if len(seq) >= 2:\n",
                "            v2_seqs.append(seq)\n",
                "    \n",
                "    v2_templates = len(miner.drain.clusters)\n",
                "    print(f\"  Sequences: {len(v2_seqs):,}, Templates: {v2_templates}\")\n",
                "    \n",
                "    # Save templates\n",
                "    with open(os.path.join(OUTPUT_DIR, \"v2_templates.json\"), 'w') as f:\n",
                "        json.dump([{'id': c.cluster_id, 'count': c.size, 'template': c.get_template()} \n",
                "                   for c in miner.drain.clusters], f, indent=2)\n",
                "else:\n",
                "    print(\"V2 not found - skipping\")\n",
                "\n",
                "# === COMBINE ALL DATA ===\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"COMBINING DATA\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "all_normal = v2_seqs + v1_normal + v3_normal\n",
                "all_failure = v1_failure + v3_failure\n",
                "\n",
                "# Use V3 for evaluation (has clean labels)\n",
                "eval_normal = v3_normal\n",
                "eval_failure = v3_failure\n",
                "\n",
                "# Pre-fail windows\n",
                "prefail_seqs = [seq[-PREFAIL_WINDOW:] if len(seq) >= PREFAIL_WINDOW else seq \n",
                "                for seq in all_failure if seq]\n",
                "\n",
                "# Vocab\n",
                "all_ids = set()\n",
                "for seq in all_normal + all_failure:\n",
                "    all_ids.update(seq)\n",
                "VOCAB_SIZE = max(all_ids) + 5\n",
                "\n",
                "print(f\"Combined Normal: {len(all_normal):,}\")\n",
                "print(f\"  V2: {len(v2_seqs):,}, V1: {len(v1_normal):,}, V3: {len(v3_normal):,}\")\n",
                "print(f\"Combined Failure: {len(all_failure):,}\")\n",
                "print(f\"Pre-fail windows: {len(prefail_seqs):,}\")\n",
                "print(f\"Vocab size: {VOCAB_SIZE}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "============================================================\n",
                        "TRAINING N-GRAM + GRAPH + BAYES\n",
                        "============================================================\n",
                        "\n",
                        "--- 4-gram Baseline ---\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "4-gram: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3245735/3245735 [01:22<00:00, 39296.61it/s] \n",
                        "Score: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 226767/226767 [00:03<00:00, 61221.74it/s]\n",
                        "Score: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29817/29817 [00:00<00:00, 68080.50it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "--- Semantic Graph (20 regex patterns) ---\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Graph: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3245735/3245735 [00:39<00:00, 81536.31it/s] \n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "  Nodes: 38583, Edges: 68713\n",
                        "  Canonical templates: 38583\n",
                        "  Top slots: []\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Graph score: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 226767/226767 [00:00<00:00, 241157.13it/s]\n",
                        "Graph score: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29817/29817 [00:00<00:00, 299616.11it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "--- Bayes Factor (full pre-fail windows) ---\n",
                        "  Pre-fail windows: 72,564 (variable length 5-20)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "3-gram: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3245735/3245735 [01:18<00:00, 41280.36it/s] \n",
                        "3-gram: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 72564/72564 [00:00<00:00, 94932.49it/s] \n",
                        "BF normal: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 226767/226767 [00:06<00:00, 34152.04it/s]\n",
                        "BF failure: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29817/29817 [00:00<00:00, 34432.79it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "============================================================\n",
                        "RESULTS\n",
                        "============================================================\n",
                        "4-gram: AUC=0.9358, F1=0.6634, P=0.9728, R=0.5033\n",
                        "Graph: AUC=0.6069, F1=0.6265, P=0.7895, R=0.5193\n",
                        "Bayes: AUC=0.9832, F1=0.7985, P=0.7159, R=0.9027\n"
                    ]
                }
            ],
            "source": [
                "#=============================================================================\n",
                "# CELL 2: N-GRAM + SEMANTIC GRAPH + BAYES FACTOR (PAPER-READY)\n",
                "#=============================================================================\n",
                "print(\"=\"*60)\n",
                "print(\"TRAINING N-GRAM + GRAPH + BAYES\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "# === ENHANCED SEMANTIC SLOT EXTRACTION (20 regex patterns) ===\n",
                "import re\n",
                "\n",
                "SLOT_PATTERNS = [\n",
                "    # Network\n",
                "    (r\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\", \"<IP>\"),\n",
                "    (r\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}:\\d+\", \"<IPPORT>\"),\n",
                "    (r\":[0-9]{2,5}(?![0-9])\", \"<PORT>\"),\n",
                "    # HDFS specific\n",
                "    (r\"blk_-?\\d+\", \"<BLK>\"),\n",
                "    (r\"BP-\\d+-\\d+-\\d+\", \"<BPOOL>\"),\n",
                "    (r\"DN[_-]?\\d+\", \"<DNODE>\"),\n",
                "    # Paths & Files  \n",
                "    (r\"/[\\w/.-]+\", \"<PATH>\"),\n",
                "    (r\"[\\w-]+\\.(?:log|txt|xml|jar|class)\", \"<FILE>\"),\n",
                "    # IDs\n",
                "    (r\"pid[=:]?\\d+\", \"<PID>\"),\n",
                "    (r\"tid[=:]?\\d+\", \"<TID>\"),\n",
                "    (r\"txid[=:]?\\d+\", \"<TXID>\"),\n",
                "    (r\"attempt_\\d+_\\d+_\\w+_\\d+_\\d+\", \"<ATTEMPT>\"),\n",
                "    (r\"job_\\d+_\\d+\", \"<JOB>\"),\n",
                "    (r\"container_\\d+_\\d+_\\d+_\\d+\", \"<CONTAINER>\"),\n",
                "    # Numbers\n",
                "    (r\"\\b\\d{10,}\\b\", \"<BIGNUM>\"),      # 10+ digit numbers\n",
                "    (r\"\\b\\d{5,9}\\b\", \"<NUM>\"),          # 5-9 digit numbers\n",
                "    (r\"\\b0x[0-9a-fA-F]+\\b\", \"<HEX>\"),\n",
                "    # Time\n",
                "    (r\"\\d{4}-\\d{2}-\\d{2}\", \"<DATE>\"),\n",
                "    (r\"\\d{2}:\\d{2}:\\d{2}\", \"<TIME>\"),\n",
                "    (r\"\\d+ms\", \"<MS>\"),\n",
                "]\n",
                "\n",
                "def canonical(t):\n",
                "    \"\"\"Apply 20 regex patterns for semantic slot extraction.\"\"\"\n",
                "    t = str(t)\n",
                "    for pattern, tag in SLOT_PATTERNS:\n",
                "        t = re.sub(pattern, tag, t)\n",
                "    return t\n",
                "\n",
                "# === N-GRAM LM ===\n",
                "class NgramLM:\n",
                "    def __init__(self, n=4, d=0.75):\n",
                "        self.n, self.d = n, d\n",
                "        self.ng, self.ctx, self.uni = defaultdict(Counter), Counter(), Counter()\n",
                "        self.total, self.vocab = 0, set()\n",
                "    \n",
                "    def fit(self, seqs):\n",
                "        for seq in tqdm(seqs, desc=f\"{self.n}-gram\"):\n",
                "            tokens = ['<S>']*(self.n-1) + [str(t) for t in seq] + ['</S>']\n",
                "            for t in seq:\n",
                "                self.uni[str(t)] += 1\n",
                "                self.total += 1\n",
                "                self.vocab.add(str(t))\n",
                "            for i in range(len(tokens)-self.n+1):\n",
                "                c = tuple(tokens[i:i+self.n-1])\n",
                "                self.ng[c][tokens[i+self.n-1]] += 1\n",
                "                self.ctx[c] += 1\n",
                "        return self\n",
                "    \n",
                "    def score(self, seq):\n",
                "        tokens = ['<S>']*(self.n-1) + [str(t) for t in seq] + ['</S>']\n",
                "        lp = 0.0\n",
                "        for i in range(len(tokens)-self.n+1):\n",
                "            c = tuple(tokens[i:i+self.n-1])\n",
                "            w = tokens[i+self.n-1]\n",
                "            cnt, cc = self.ng[c].get(w,0), self.ctx.get(c,0)\n",
                "            if cc > 0:\n",
                "                p = max(cnt-self.d,0)/cc + (self.d*len(self.ng[c]))/cc * (self.uni.get(w,0)+1)/(self.total+len(self.vocab))\n",
                "            else:\n",
                "                p = (self.uni.get(w,0)+1)/(self.total+len(self.vocab))\n",
                "            lp += np.log(p+1e-10)\n",
                "        return lp\n",
                "    \n",
                "    def anomaly_all(self, seqs):\n",
                "        return np.array([-self.score(s) for s in tqdm(seqs, desc=\"Score\")])\n",
                "\n",
                "# === SEMANTIC GRAPH WITH ENHANCED SLOTS ===\n",
                "class SemanticGraph:\n",
                "    def __init__(self):\n",
                "        self.G = nx.DiGraph()\n",
                "        self.tid_to_sem = {}\n",
                "        self.sem_to_id = {}\n",
                "        self.next_id = 0\n",
                "        self.slot_stats = Counter()  # Track slot usage\n",
                "    \n",
                "    def _to_sem(self, tid):\n",
                "        if tid not in self.tid_to_sem:\n",
                "            c = canonical(f\"T{tid}\")\n",
                "            # Count slots used\n",
                "            for _, tag in SLOT_PATTERNS:\n",
                "                if tag in c:\n",
                "                    self.slot_stats[tag] += 1\n",
                "            if c not in self.sem_to_id:\n",
                "                self.sem_to_id[c] = self.next_id\n",
                "                self.next_id += 1\n",
                "            self.tid_to_sem[tid] = self.sem_to_id[c]\n",
                "        return self.tid_to_sem[tid]\n",
                "    \n",
                "    def fit(self, seqs):\n",
                "        for seq in tqdm(seqs, desc=\"Graph\"):\n",
                "            sseq = [self._to_sem(t) for t in seq]\n",
                "            for a, b in zip(sseq[:-1], sseq[1:]):\n",
                "                if self.G.has_edge(a, b): self.G[a][b]['w'] += 1\n",
                "                else: self.G.add_edge(a, b, w=1)\n",
                "        V = len(self.G.nodes())\n",
                "        for u in self.G.nodes():\n",
                "            out = sum(self.G[u][v]['w'] for v in self.G.successors(u))\n",
                "            for v in self.G.successors(u):\n",
                "                self.G[u][v]['p'] = (self.G[u][v]['w'] + 1) / (out + V)\n",
                "        print(f\"  Nodes: {self.G.number_of_nodes()}, Edges: {self.G.number_of_edges()}\")\n",
                "        print(f\"  Canonical templates: {len(self.sem_to_id)}\")\n",
                "        print(f\"  Top slots: {self.slot_stats.most_common(5)}\")\n",
                "        return self\n",
                "    \n",
                "    def score_all(self, seqs):\n",
                "        V = len(self.G.nodes()) or 1\n",
                "        scores = []\n",
                "        for seq in tqdm(seqs, desc=\"Graph score\"):\n",
                "            if len(seq) < 2: scores.append(0.0); continue\n",
                "            sseq = [self._to_sem(t) for t in seq]\n",
                "            s = sum(-math.log(self.G[a][b]['p'] if self.G.has_edge(a,b) else 1/(V**2+1) + 1e-10) \n",
                "                    for a, b in zip(sseq[:-1], sseq[1:]))\n",
                "            scores.append(s / max(len(sseq)-1, 1))\n",
                "        return np.array(scores)\n",
                "\n",
                "# === FULL PRE-FAIL WINDOWS (variable length) ===\n",
                "def extract_prefail_windows(failure_seqs, min_window=5, max_window=20):\n",
                "    \"\"\"Extract pre-fail windows of varying lengths for richer patterns.\"\"\"\n",
                "    windows = []\n",
                "    for seq in failure_seqs:\n",
                "        if len(seq) >= min_window:\n",
                "            # Use multiple window sizes\n",
                "            for w in [min_window, 10, max_window]:\n",
                "                if len(seq) >= w:\n",
                "                    windows.append(seq[-w:])\n",
                "        elif len(seq) > 0:\n",
                "            windows.append(seq)\n",
                "    return windows\n",
                "\n",
                "# Train\n",
                "print(\"\\n--- 4-gram Baseline ---\")\n",
                "lm_base = NgramLM(n=4).fit(all_normal)\n",
                "normal_base = lm_base.anomaly_all(eval_normal)\n",
                "failure_base = lm_base.anomaly_all(eval_failure)\n",
                "\n",
                "print(\"\\n--- Semantic Graph (20 regex patterns) ---\")\n",
                "graph = SemanticGraph().fit(all_normal)\n",
                "normal_graph = graph.score_all(eval_normal)\n",
                "failure_graph = graph.score_all(eval_failure)\n",
                "\n",
                "print(\"\\n--- Bayes Factor (full pre-fail windows) ---\")\n",
                "# Extract FULL pre-fail windows with variable lengths\n",
                "full_prefail = extract_prefail_windows(all_failure, min_window=5, max_window=20)\n",
                "print(f\"  Pre-fail windows: {len(full_prefail):,} (variable length 5-20)\")\n",
                "\n",
                "lm_normal = NgramLM(n=3).fit(all_normal)\n",
                "lm_prefail = NgramLM(n=3).fit(full_prefail)  # Trained on FULL pre-fail windows\n",
                "normal_bf = np.array([lm_prefail.score(s) - lm_normal.score(s) for s in tqdm(eval_normal, desc=\"BF normal\")])\n",
                "failure_bf = np.array([lm_prefail.score(s) - lm_normal.score(s) for s in tqdm(eval_failure, desc=\"BF failure\")])\n",
                "\n",
                "# Evaluate\n",
                "def evaluate(n_scores, f_scores, name):\n",
                "    all_s = np.concatenate([n_scores, f_scores])\n",
                "    all_l = np.concatenate([np.zeros(len(n_scores)), np.ones(len(f_scores))])\n",
                "    auc = roc_auc_score(all_l, all_s)\n",
                "    p, r, _ = precision_recall_curve(all_l, all_s)\n",
                "    f1s = 2*p*r/(p+r+1e-10)\n",
                "    idx = np.argmax(f1s)\n",
                "    print(f\"{name}: AUC={auc:.4f}, F1={f1s[idx]:.4f}, P={p[idx]:.4f}, R={r[idx]:.4f}\")\n",
                "    return {'auc': float(auc), 'f1': float(f1s[idx]), 'p': float(p[idx]), 'r': float(r[idx])}\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"RESULTS\")\n",
                "print(\"=\"*60)\n",
                "base_r = evaluate(normal_base, failure_base, \"4-gram\")\n",
                "graph_r = evaluate(normal_graph, failure_graph, \"Graph\")\n",
                "bf_r = evaluate(normal_bf, failure_bf, \"Bayes\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Loading: 2460745it [00:14, 174449.61it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loaded 2,460,745 sequences\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n"
                    ]
                }
            ],
            "source": [
                "\n",
                "import json\n",
                "from tqdm import tqdm\n",
                "# Load sequences tá»« file saved\n",
                "all_normal = []\n",
                "with open('output/v2_sequences.jsonl', 'r') as f:\n",
                "    for line in tqdm(f, desc=\"Loading\"):\n",
                "        all_normal.append(json.loads(line)['seq'])\n",
                "print(f\"Loaded {len(all_normal):,} sequences\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "============================================================\n",
                        "TRAINING LOGBERT-VHM\n",
                        "============================================================\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "âœ… Loaded checkpoint epoch 11 (loss=0.0757)\n",
                        "Training: epochs 12-15\n",
                        "Batches: 19,225, LR: 3e-05\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Ep 12/15:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 8278/19225 [29:25<38:57,  4.68it/s, loss=0.0615]  "
                    ]
                }
            ],
            "source": [
                "#=============================================================================\n",
                "# CELL 3: LOGBERT-VHM - RESUME Tá»ª CHECKPOINT\n",
                "#=============================================================================\n",
                "print(\"=\"*60)\n",
                "print(\"TRAINING LOGBERT-VHM\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "import os, time\n",
                "\n",
                "PAD, CLS, MASK, SEP, OFF = 0, 1, 2, 3, 4\n",
                "CKPT_DIR = \"checkpoints\"\n",
                "os.makedirs(CKPT_DIR, exist_ok=True)\n",
                "os.makedirs('output', exist_ok=True)\n",
                "\n",
                "class LogDS(Dataset):\n",
                "    def __init__(self, seqs, ml=CONTEXT_LEN, mr=MASK_RATIO):\n",
                "        self.seqs, self.ml, self.mr = seqs, ml, mr\n",
                "    def __len__(self): return len(self.seqs)\n",
                "    def __getitem__(self, i):\n",
                "        s = [min(t+OFF, VOCAB_SIZE-1) for t in self.seqs[i][:self.ml-2]]\n",
                "        tok = [CLS]+s+[SEP]+[PAD]*(self.ml-len(s)-2)\n",
                "        inp, lab = tok.copy(), [-100]*len(tok)\n",
                "        for j in range(1, len(s)+1):\n",
                "            if random.random() < self.mr: lab[j], inp[j] = inp[j], MASK\n",
                "        return {'ids': torch.tensor(inp), 'lab': torch.tensor(lab),\n",
                "                'mask': torch.tensor([1 if t!=PAD else 0 for t in tok])}\n",
                "\n",
                "class LogBERT(nn.Module):\n",
                "    def __init__(self, vs, dm=D_MODEL, nh=N_HEADS, nl=N_LAYERS, ml=CONTEXT_LEN):\n",
                "        super().__init__()\n",
                "        self.tok = nn.Embedding(vs, dm, padding_idx=PAD)\n",
                "        self.pos = nn.Embedding(ml, dm)\n",
                "        self.drop = nn.Dropout(0.1)\n",
                "        el = nn.TransformerEncoderLayer(dm, nh, dm*4, 0.1, 'gelu', batch_first=True)\n",
                "        self.enc = nn.TransformerEncoder(el, nl)\n",
                "        self.head = nn.Linear(dm, vs)\n",
                "        self.register_buffer('ctr', torch.zeros(dm))\n",
                "        self.ci = False\n",
                "    \n",
                "    def forward(self, ids, mask=None):\n",
                "        x = self.tok(ids) + self.pos(torch.arange(ids.size(1), device=ids.device))\n",
                "        h = self.enc(self.drop(x), src_key_padding_mask=(mask==0) if mask is not None else None)\n",
                "        return self.head(h), h[:,0,:]\n",
                "    \n",
                "    def loss(self, lg, lb, cls):\n",
                "        mlm = F.cross_entropy(lg.view(-1, lg.size(-1)), lb.view(-1), ignore_index=-100)\n",
                "        vhm = torch.mean((cls-self.ctr)**2) if self.ci else 0.0\n",
                "        return mlm + LAMBDA_VHM*vhm\n",
                "    \n",
                "    def upd(self, e):\n",
                "        with torch.no_grad():\n",
                "            bc = e.mean(0)\n",
                "            self.ctr = bc if not self.ci else 0.9*self.ctr + 0.1*bc\n",
                "            self.ci = True\n",
                "\n",
                "# === CHECK & LOAD CHECKPOINT ===\n",
                "RESUME_EPOCH = 11\n",
                "TOTAL_EPOCHS = 15\n",
                "ckpt_file = f\"{CKPT_DIR}/logbert_ep{RESUME_EPOCH}.pt\"\n",
                "\n",
                "model = LogBERT(VOCAB_SIZE).to(device)\n",
                "\n",
                "if os.path.exists(ckpt_file):\n",
                "    ckpt = torch.load(ckpt_file, map_location=device)\n",
                "    model.load_state_dict(ckpt['model_state_dict'])\n",
                "    model.ctr = ckpt['center']\n",
                "    model.ci = True\n",
                "    START_EPOCH = RESUME_EPOCH + 1\n",
                "    best_loss = ckpt['loss']\n",
                "    LR = 3e-5  # Lower LR for fine-tuning\n",
                "    print(f\"âœ… Loaded checkpoint epoch {RESUME_EPOCH} (loss={best_loss:.4f})\")\n",
                "else:\n",
                "    START_EPOCH = 1\n",
                "    best_loss = float('inf')\n",
                "    LR = 1e-4\n",
                "    print(\"No checkpoint, training from scratch\")\n",
                "\n",
                "# === TRAINING ===\n",
                "scaler = torch.amp.GradScaler('cuda')\n",
                "opt = torch.optim.AdamW(model.parameters(), lr=LR)\n",
                "loader = DataLoader(LogDS(all_normal), batch_size=128, shuffle=True, num_workers=2, pin_memory=True)\n",
                "\n",
                "print(f\"Training: epochs {START_EPOCH}-{TOTAL_EPOCHS}\")\n",
                "print(f\"Batches: {len(loader):,}, LR: {LR}\")\n",
                "start = time.time()\n",
                "\n",
                "for ep in range(START_EPOCH, TOTAL_EPOCHS + 1):\n",
                "    model.train()\n",
                "    tl = 0\n",
                "    pbar = tqdm(loader, desc=f\"Ep {ep}/{TOTAL_EPOCHS}\")\n",
                "    \n",
                "    for b in pbar:\n",
                "        ids, lab, mask = b['ids'].to(device), b['lab'].to(device), b['mask'].to(device)\n",
                "        opt.zero_grad()\n",
                "        with torch.amp.autocast('cuda'):\n",
                "            lg, cls = model(ids, mask)\n",
                "            loss = model.loss(lg, lab, cls)\n",
                "        scaler.scale(loss).backward()\n",
                "        scaler.step(opt)\n",
                "        scaler.update()\n",
                "        model.upd(cls.detach())\n",
                "        tl += loss.item()\n",
                "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
                "    \n",
                "    avg = tl / len(loader)\n",
                "    print(f\"Epoch {ep}: {avg:.4f}\")\n",
                "    \n",
                "    if avg < best_loss - 1e-4:\n",
                "        best_loss = avg\n",
                "        torch.save(model.state_dict(), f\"{CKPT_DIR}/logbert_best.pt\")\n",
                "        print(\"  âœ“ Best!\")\n",
                "    \n",
                "    torch.save({\"epoch\": ep, \"model_state_dict\": model.state_dict(),\n",
                "                \"center\": model.ctr, \"loss\": avg}, f\"{CKPT_DIR}/logbert_ep{ep}.pt\")\n",
                "\n",
                "# === SCORE & SAVE ===\n",
                "@torch.no_grad()\n",
                "def bert_score(seqs):\n",
                "    model.eval()\n",
                "    ld = DataLoader(LogDS(seqs, mr=0), batch_size=128, shuffle=False)\n",
                "    sc = []\n",
                "    for b in tqdm(ld, desc=\"BERT score\"):\n",
                "        _, cls = model(b['ids'].to(device), b['mask'].to(device))\n",
                "        sc.extend(torch.sum((cls-model.ctr)**2, dim=1).cpu().numpy())\n",
                "    return np.array(sc)\n",
                "\n",
                "normal_bert = bert_score(eval_normal)\n",
                "failure_bert = bert_score(eval_failure)\n",
                "bert_r = evaluate(normal_bert, failure_bert, \"LogBERT\")\n",
                "\n",
                "torch.save(model.state_dict(), 'output/logbert.pt')\n",
                "print(f\"\\nâœ“ Done in {(time.time()-start)/60:.1f} min\")\n",
                "print(f\"âœ“ Saved: output/logbert.pt\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "============================================================\n",
                        "LOADING SCORES & ENSEMBLE\n",
                        "============================================================\n",
                        "âœ… Loaded: normal=226,767, failure=29,817\n",
                        "  (0, 0.5, 0.5): AUC=0.9742, F1=0.7530\n",
                        "  (0, 0.6, 0.4): AUC=0.9761, F1=0.7576\n",
                        "  (0.1, 0.5, 0.4): AUC=0.9726, F1=0.7402\n",
                        "  (0.1, 0.6, 0.3): AUC=0.9746, F1=0.7488\n",
                        "  (0.2, 0.4, 0.4): AUC=0.9707, F1=0.7312\n",
                        "\n",
                        "======================================================================\n",
                        "ðŸŽ¯ FINAL RESULTS\n",
                        "======================================================================\n",
                        "\n",
                        "Method                    AUC       F1        P        R\n",
                        "-------------------------------------------------------\n",
                        "Semantic Graph         0.5782   0.5679   0.9639   0.4025\n",
                        "Bayes Factor           0.9832   0.7985   0.7160   0.9026\n",
                        "LogBERT-VHM            0.8738   0.5536   0.6046   0.5105\n",
                        "-------------------------------------------------------\n",
                        "ENSEMBLE               0.9761   0.7576\n",
                        "  Weights: G=0, B=0.6, L=0.4\n",
                        "======================================================================\n",
                        "\n",
                        "âœ“ Saved: output/results.json\n",
                        "ðŸŽ‰ COMPLETE!\n"
                    ]
                }
            ],
            "source": [
                "#=============================================================================\n",
                "# CELL 4: LOAD SCORES + ENSEMBLE + FINAL RESULTS\n",
                "#=============================================================================\n",
                "import numpy as np\n",
                "from sklearn.metrics import precision_recall_curve, roc_auc_score\n",
                "import json\n",
                "import os\n",
                "\n",
                "OUTPUT_DIR = \"output\"\n",
                "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
                "\n",
                "print(\"=\"*60)\n",
                "print(\"LOADING SCORES & ENSEMBLE\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "# === LOAD SCORES Tá»ª FILE ===\n",
                "normal_graph = np.load(f\"{OUTPUT_DIR}/normal_graph_scores.npy\")\n",
                "failure_graph = np.load(f\"{OUTPUT_DIR}/failure_graph_scores.npy\")\n",
                "normal_bf = np.load(f\"{OUTPUT_DIR}/normal_bf_scores.npy\")\n",
                "failure_bf = np.load(f\"{OUTPUT_DIR}/failure_bf_scores.npy\")\n",
                "normal_bert = np.load(f\"{OUTPUT_DIR}/normal_bert_scores.npy\")\n",
                "failure_bert = np.load(f\"{OUTPUT_DIR}/failure_bert_scores.npy\")\n",
                "print(f\"âœ… Loaded: normal={len(normal_graph):,}, failure={len(failure_graph):,}\")\n",
                "\n",
                "# === ENSEMBLE ===\n",
                "def norm(s): return (s-s.min())/(s.max()-s.min()+1e-10)\n",
                "\n",
                "ag = norm(np.concatenate([normal_graph, failure_graph]))\n",
                "ab = norm(np.concatenate([normal_bf, failure_bf]))\n",
                "al = norm(np.concatenate([normal_bert, failure_bert]))\n",
                "labs = np.concatenate([np.zeros(len(normal_graph)), np.ones(len(failure_graph))])\n",
                "\n",
                "ws = [(0,0.5,0.5),(0,0.6,0.4),(0.1,0.5,0.4),(0.1,0.6,0.3),(0.2,0.4,0.4)]\n",
                "bf1, bw, bauc = 0, None, 0\n",
                "\n",
                "for w in ws:\n",
                "    e = w[0]*ag + w[1]*ab + w[2]*al\n",
                "    auc = roc_auc_score(labs, e)\n",
                "    p, r, _ = precision_recall_curve(labs, e)\n",
                "    f1 = np.max(2*p*r/(p+r+1e-10))\n",
                "    print(f\"  {w}: AUC={auc:.4f}, F1={f1:.4f}\")\n",
                "    if f1 > bf1: bf1, bw, bauc = f1, w, auc\n",
                "\n",
                "# === EVALUATE METHODS ===\n",
                "def evaluate(n_scores, f_scores):\n",
                "    all_s = np.concatenate([n_scores, f_scores])\n",
                "    all_l = np.concatenate([np.zeros(len(n_scores)), np.ones(len(f_scores))])\n",
                "    auc = roc_auc_score(all_l, all_s)\n",
                "    p, r, _ = precision_recall_curve(all_l, all_s)\n",
                "    f1s = 2*p*r/(p+r+1e-10)\n",
                "    idx = np.argmax(f1s)\n",
                "    return {'auc': float(auc), 'f1': float(f1s[idx]), 'p': float(p[idx]), 'r': float(r[idx])}\n",
                "\n",
                "graph_r = evaluate(normal_graph, failure_graph)\n",
                "bf_r = evaluate(normal_bf, failure_bf)\n",
                "bert_r = evaluate(normal_bert, failure_bert)\n",
                "\n",
                "# === FINAL RESULTS ===\n",
                "print(\"\\n\" + \"=\"*70)\n",
                "print(\"ðŸŽ¯ FINAL RESULTS\")\n",
                "print(\"=\"*70)\n",
                "\n",
                "print(f\"\\n{'Method':<20} {'AUC':>8} {'F1':>8} {'P':>8} {'R':>8}\")\n",
                "print(\"-\"*55)\n",
                "print(f\"{'Semantic Graph':<20} {graph_r['auc']:>8.4f} {graph_r['f1']:>8.4f} {graph_r['p']:>8.4f} {graph_r['r']:>8.4f}\")\n",
                "print(f\"{'Bayes Factor':<20} {bf_r['auc']:>8.4f} {bf_r['f1']:>8.4f} {bf_r['p']:>8.4f} {bf_r['r']:>8.4f}\")\n",
                "print(f\"{'LogBERT-VHM':<20} {bert_r['auc']:>8.4f} {bert_r['f1']:>8.4f} {bert_r['p']:>8.4f} {bert_r['r']:>8.4f}\")\n",
                "print(\"-\"*55)\n",
                "print(f\"{'ENSEMBLE':<20} {bauc:>8.4f} {bf1:>8.4f}\")\n",
                "print(f\"  Weights: G={bw[0]}, B={bw[1]}, L={bw[2]}\")\n",
                "print(\"=\"*70)\n",
                "\n",
                "# === SAVE ===\n",
                "results = {\n",
                "    'graph': graph_r, 'bayes': bf_r, 'bert': bert_r,\n",
                "    'ensemble': {'weights': list(bw), 'auc': float(bauc), 'f1': float(bf1)}\n",
                "}\n",
                "with open(f\"{OUTPUT_DIR}/results.json\", 'w') as f:\n",
                "    json.dump(results, f, indent=2)\n",
                "\n",
                "print(f\"\\nâœ“ Saved: {OUTPUT_DIR}/results.json\")\n",
                "print(\"ðŸŽ‰ COMPLETE!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "VOCAB_SIZE: 38735\n",
                        "============================================================\n",
                        "SIMCLR CONTRASTIVE FINE-TUNING\n",
                        "============================================================\n",
                        "âœ… Loaded model from epoch 10\n",
                        "Loaded 50000 sequences\n",
                        "SimCLR training: 50,000 sequences, 10 epochs\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "SimCLR 1/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 781/781 [02:58<00:00,  4.38it/s, loss=3.6992]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "SimCLR Epoch 1: 4.1375\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "SimCLR 2/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 781/781 [02:57<00:00,  4.39it/s, loss=2.9855]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "SimCLR Epoch 2: 3.2172\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "SimCLR 3/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 781/781 [02:57<00:00,  4.39it/s, loss=2.8048]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "SimCLR Epoch 3: 2.8123\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "SimCLR 4/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 781/781 [02:57<00:00,  4.39it/s, loss=2.3806]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "SimCLR Epoch 4: 2.6121\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "SimCLR 5/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 781/781 [02:57<00:00,  4.39it/s, loss=2.2737]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "SimCLR Epoch 5: 2.4510\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "SimCLR 6/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 781/781 [02:57<00:00,  4.39it/s, loss=2.5419]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "SimCLR Epoch 6: 2.3613\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "SimCLR 7/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 781/781 [02:57<00:00,  4.39it/s, loss=2.3125]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "SimCLR Epoch 7: 2.3147\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "SimCLR 8/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 781/781 [02:57<00:00,  4.39it/s, loss=2.1542]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "SimCLR Epoch 8: 2.2679\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "SimCLR 9/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 781/781 [02:57<00:00,  4.39it/s, loss=2.3058]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "SimCLR Epoch 9: 2.2445\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "SimCLR 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 781/781 [02:57<00:00,  4.39it/s, loss=2.0904]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "SimCLR Epoch 10: 2.2292\n",
                        "\n",
                        "âœ“ Saved output/logbert_simclr.pt\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n"
                    ]
                }
            ],
            "source": [
                "#=============================================================================\n",
                "# CELL 5: SIMCLR CONTRASTIVE FINE-TUNING\n",
                "#=============================================================================\n",
                "import os\n",
                "import random\n",
                "import numpy as np\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "from tqdm import tqdm\n",
                "from sklearn.metrics import precision_recall_curve, roc_auc_score\n",
                "\n",
                "# Device\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "\n",
                "# Constants\n",
                "PAD, CLS, MASK, SEP, OFF = 0, 1, 2, 3, 4\n",
                "CONTEXT_LEN = 128\n",
                "D_MODEL, N_HEADS, N_LAYERS = 256, 8, 4\n",
                "BATCH_SIZE = 64\n",
                "SIMCLR_EPOCHS = 10\n",
                "TEMPERATURE = 0.1\n",
                "OUTPUT_DIR = \"output\"\n",
                "CKPT_DIR = \"checkpoints\"\n",
                "\n",
                "# Load VOCAB_SIZE from checkpoint\n",
                "ckpt = torch.load(f\"{CKPT_DIR}/logbert_ep10.pt\", map_location=device)\n",
                "VOCAB_SIZE = ckpt['model_state_dict']['tok.weight'].shape[0]\n",
                "print(f\"VOCAB_SIZE: {VOCAB_SIZE}\")\n",
                "\n",
                "print(\"=\"*60)\n",
                "print(\"SIMCLR CONTRASTIVE FINE-TUNING\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "# === LOGBERT MODEL ===\n",
                "class LogBERT(nn.Module):\n",
                "    def __init__(self, vs, dm=D_MODEL, nh=N_HEADS, nl=N_LAYERS, ml=CONTEXT_LEN):\n",
                "        super().__init__()\n",
                "        self.tok = nn.Embedding(vs, dm, padding_idx=PAD)\n",
                "        self.pos = nn.Embedding(ml, dm)\n",
                "        self.drop = nn.Dropout(0.1)\n",
                "        el = nn.TransformerEncoderLayer(dm, nh, dm*4, 0.1, 'gelu', batch_first=True)\n",
                "        self.enc = nn.TransformerEncoder(el, nl)\n",
                "        self.head = nn.Linear(dm, vs)\n",
                "        self.register_buffer('ctr', torch.zeros(dm))\n",
                "        self.ci = False\n",
                "    \n",
                "    def forward(self, ids, mask=None):\n",
                "        x = self.tok(ids) + self.pos(torch.arange(ids.size(1), device=ids.device))\n",
                "        h = self.enc(self.drop(x), src_key_padding_mask=(mask==0) if mask is not None else None)\n",
                "        return self.head(h), h[:,0,:]\n",
                "\n",
                "# Load model\n",
                "model = LogBERT(VOCAB_SIZE).to(device)\n",
                "model.load_state_dict(ckpt['model_state_dict'])\n",
                "model.ctr = ckpt['center']\n",
                "model.ci = True\n",
                "print(f\"âœ… Loaded model from epoch {ckpt['epoch']}\")\n",
                "\n",
                "# === LOAD DATA (sample) ===\n",
                "# Táº¡o dummy data Ä‘á»ƒ test, hoáº·c load tá»« file\n",
                "try:\n",
                "    import json\n",
                "    with open(f\"{OUTPUT_DIR}/v2_sequences.jsonl\", 'r') as f:\n",
                "        all_normal = [json.loads(line)['seq'] for line in f][:50000]\n",
                "    print(f\"Loaded {len(all_normal)} sequences\")\n",
                "except:\n",
                "    print(\"âš ï¸ Cannot load sequences, using random data for demo\")\n",
                "    all_normal = [[random.randint(0, 100) for _ in range(50)] for _ in range(10000)]\n",
                "\n",
                "# === AUGMENTATION ===\n",
                "def augment(seq, aug_type):\n",
                "    seq = list(seq)\n",
                "    if aug_type == 'mask':\n",
                "        for i in range(len(seq)):\n",
                "            if random.random() < 0.15: seq[i] = MASK\n",
                "    elif aug_type == 'drop':\n",
                "        seq = [t for t in seq if random.random() > 0.15] or [seq[0]]\n",
                "    elif aug_type == 'shuffle':\n",
                "        for i in range(0, len(seq)-2, 3):\n",
                "            chunk = seq[i:i+3]\n",
                "            random.shuffle(chunk)\n",
                "            seq[i:i+3] = chunk\n",
                "    elif aug_type == 'crop':\n",
                "        n = max(1, int(len(seq) * random.uniform(0.7, 0.9)))\n",
                "        s = random.randint(0, len(seq) - n)\n",
                "        seq = seq[s:s+n]\n",
                "    return seq\n",
                "\n",
                "class ContrastiveDS(Dataset):\n",
                "    def __init__(self, seqs, ml=CONTEXT_LEN):\n",
                "        self.seqs, self.ml = seqs, ml\n",
                "        self.augs = ['mask', 'drop', 'shuffle', 'crop']\n",
                "    def __len__(self): return len(self.seqs)\n",
                "    def _tok(self, seq):\n",
                "        s = [min(t+OFF, VOCAB_SIZE-1) for t in seq[:self.ml-2]]\n",
                "        tok = [CLS]+s+[SEP]+[PAD]*(self.ml-len(s)-2)\n",
                "        return torch.tensor(tok), torch.tensor([1 if t!=PAD else 0 for t in tok])\n",
                "    def __getitem__(self, i):\n",
                "        seq = self.seqs[i]\n",
                "        s1 = augment(seq, random.choice(self.augs))\n",
                "        s2 = augment(seq, random.choice(self.augs))\n",
                "        ids1, m1 = self._tok(s1)\n",
                "        ids2, m2 = self._tok(s2)\n",
                "        return {'ids1': ids1, 'm1': m1, 'ids2': ids2, 'm2': m2}\n",
                "\n",
                "# === PROJECTION HEAD ===\n",
                "class ProjHead(nn.Module):\n",
                "    def __init__(self, din=D_MODEL, dh=512, dout=128):\n",
                "        super().__init__()\n",
                "        self.net = nn.Sequential(nn.Linear(din, dh), nn.ReLU(), nn.Linear(dh, dout))\n",
                "    def forward(self, x): return F.normalize(self.net(x), dim=1)\n",
                "\n",
                "# === NT-XENT LOSS ===\n",
                "def nt_xent(z1, z2, temp=TEMPERATURE):\n",
                "    B = z1.size(0)\n",
                "    z = torch.cat([z1, z2], dim=0)\n",
                "    sim = torch.mm(z, z.t()) / temp\n",
                "    mask = torch.eye(2*B, device=z.device).bool()\n",
                "    sim.masked_fill_(mask, float('-inf'))\n",
                "    labels = torch.cat([torch.arange(B, 2*B), torch.arange(B)]).to(z.device)\n",
                "    return F.cross_entropy(sim, labels)\n",
                "\n",
                "# === TRAIN SIMCLR ===\n",
                "proj = ProjHead().to(device)\n",
                "simclr_loader = DataLoader(ContrastiveDS(all_normal[:50000]), batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
                "simclr_opt = torch.optim.AdamW(list(model.parameters()) + list(proj.parameters()), lr=1e-5)\n",
                "\n",
                "print(f\"SimCLR training: {min(50000, len(all_normal)):,} sequences, {SIMCLR_EPOCHS} epochs\")\n",
                "\n",
                "for ep in range(SIMCLR_EPOCHS):\n",
                "    model.train(); proj.train()\n",
                "    tl = 0\n",
                "    pbar = tqdm(simclr_loader, desc=f\"SimCLR {ep+1}/{SIMCLR_EPOCHS}\")\n",
                "    for b in pbar:\n",
                "        ids1, m1 = b['ids1'].to(device), b['m1'].to(device)\n",
                "        ids2, m2 = b['ids2'].to(device), b['m2'].to(device)\n",
                "        simclr_opt.zero_grad()\n",
                "        _, e1 = model(ids1, m1)\n",
                "        _, e2 = model(ids2, m2)\n",
                "        loss = nt_xent(proj(e1), proj(e2))\n",
                "        loss.backward()\n",
                "        simclr_opt.step()\n",
                "        tl += loss.item()\n",
                "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
                "    print(f\"SimCLR Epoch {ep+1}: {tl/len(simclr_loader):.4f}\")\n",
                "\n",
                "torch.save(model.state_dict(), f\"{OUTPUT_DIR}/logbert_simclr.pt\")\n",
                "print(f\"\\nâœ“ Saved {OUTPUT_DIR}/logbert_simclr.pt\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "============================================================\n",
                        "BGL CROSS-DOMAIN ADAPTATION\n",
                        "============================================================\n",
                        "VOCAB_SIZE from checkpoint: 38735\n",
                        "âœ… Found BGL: /teamspace/studios/this_studio/content/BGL.log\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Parsing BGL: 17238it [00:00, 72427.46it/s]"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Parsing BGL: 200001it [00:03, 59409.30it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Templates extracted: 36334\n",
                        "BGL Normal: 9,864, Failure: 136\n",
                        "âœ… Loaded SimCLR model\n",
                        "\n",
                        "BGL training: 9,864 sequences, 20 epochs\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "BGL 1/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:18<00:00,  8.39it/s, loss=8.1154] \n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "BGL Epoch 1: 10.1023\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "BGL 2/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:18<00:00,  8.34it/s, loss=9.3117] \n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "BGL Epoch 2: 9.3735\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "BGL 3/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:18<00:00,  8.35it/s, loss=10.5298]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "BGL Epoch 3: 9.2904\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "BGL 4/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:18<00:00,  8.33it/s, loss=10.2103]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "BGL Epoch 4: 9.1740\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "BGL 5/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:18<00:00,  8.32it/s, loss=10.3338]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "BGL Epoch 5: 9.1415\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "BGL 6/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:18<00:00,  8.30it/s, loss=10.1811]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "BGL Epoch 6: 9.0792\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "BGL 7/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:18<00:00,  8.32it/s, loss=9.8472] \n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "BGL Epoch 7: 9.0394\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "BGL 8/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:18<00:00,  8.34it/s, loss=7.8758]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "BGL Epoch 8: 8.9884\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "BGL 9/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:18<00:00,  8.34it/s, loss=9.1507]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "BGL Epoch 9: 9.0035\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "BGL 10/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:18<00:00,  8.38it/s, loss=9.8522]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "BGL Epoch 10: 9.0054\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "BGL 11/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:18<00:00,  8.37it/s, loss=7.4767]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "BGL Epoch 11: 8.9660\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "BGL 12/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:18<00:00,  8.31it/s, loss=10.1053]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "BGL Epoch 12: 8.9989\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "BGL 13/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:18<00:00,  8.37it/s, loss=9.7924] \n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "BGL Epoch 13: 8.9990\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "BGL 14/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:18<00:00,  8.36it/s, loss=5.9076] \n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "BGL Epoch 14: 8.9354\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "BGL 15/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:18<00:00,  8.30it/s, loss=10.2276]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "BGL Epoch 15: 8.9526\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "BGL 16/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:18<00:00,  8.36it/s, loss=9.0411]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "BGL Epoch 16: 8.9205\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "BGL 17/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:18<00:00,  8.33it/s, loss=6.7652]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "BGL Epoch 17: 8.9267\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "BGL 18/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:18<00:00,  8.38it/s, loss=8.4758]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "BGL Epoch 18: 8.9332\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "BGL 19/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 155/155 [00:18<00:00,  8.36it/s, loss=6.0725]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "BGL Epoch 19: 8.9228\n",
                        "  Early stop at epoch 19\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Score: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [00:04<00:00,  8.26it/s]\n",
                        "Score: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 14.98it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "======================================================================\n",
                        "ðŸŽ¯ BGL RESULTS\n",
                        "======================================================================\n",
                        "BGL: AUC=0.9513, F1=0.8907, P=0.9910, R=0.8088\n",
                        "Threshold: 2.1885\n",
                        "\n",
                        "âœ“ Saved: output/logbert_bgl.pt, bgl_results.json\n"
                    ]
                }
            ],
            "source": [
                "#=============================================================================\n",
                "# CELL 6: BGL CROSS-DOMAIN ADAPTATION (PROPERLY FIXED)\n",
                "#=============================================================================\n",
                "import os\n",
                "import random\n",
                "import json\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "from tqdm import tqdm\n",
                "from sklearn.metrics import precision_recall_curve, roc_auc_score\n",
                "\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "PAD, CLS, MASK, SEP, OFF = 0, 1, 2, 3, 4\n",
                "CONTEXT_LEN, D_MODEL, N_HEADS, N_LAYERS = 128, 256, 8, 4\n",
                "BATCH_SIZE = 64\n",
                "MASK_RATIO = 0.15\n",
                "LAMBDA_VHM = 0.5\n",
                "BGL_EPOCHS = 20\n",
                "EARLY_STOP = 3\n",
                "OUTPUT_DIR = \"output\"\n",
                "CKPT_DIR = \"checkpoints\"\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"BGL CROSS-DOMAIN ADAPTATION\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "# === LOAD MODEL & GET VOCAB_SIZE ===\n",
                "ckpt = torch.load(f\"{OUTPUT_DIR}/logbert_simclr.pt\", map_location=device)\n",
                "VOCAB_SIZE = ckpt['tok.weight'].shape[0]\n",
                "print(f\"VOCAB_SIZE from checkpoint: {VOCAB_SIZE}\")\n",
                "\n",
                "# === BGL DATA ===\n",
                "BGL_FILE = \"/teamspace/studios/this_studio/content/BGL.log\"\n",
                "if not os.path.exists(BGL_FILE):\n",
                "    raise FileNotFoundError(f\"âŒ BGL data not found: {BGL_FILE}\")\n",
                "\n",
                "print(f\"âœ… Found BGL: {BGL_FILE}\")\n",
                "\n",
                "# Parse BGL vá»›i Drain-style template (simplified)\n",
                "from collections import defaultdict\n",
                "template_map = {}\n",
                "next_tid = 0\n",
                "\n",
                "def get_template_id(line):\n",
                "    global next_tid\n",
                "    # Simple template: remove numbers, keep structure\n",
                "    import re\n",
                "    template = re.sub(r'\\b\\d+\\b', '<NUM>', line)\n",
                "    template = re.sub(r'\\b[0-9a-fA-F]{8,}\\b', '<HEX>', template)\n",
                "    template = re.sub(r'/\\S+', '<PATH>', template)\n",
                "    \n",
                "    if template not in template_map:\n",
                "        template_map[template] = min(next_tid, VOCAB_SIZE - OFF - 1)  # Clamp to vocab\n",
                "        next_tid += 1\n",
                "    return template_map[template]\n",
                "\n",
                "bgl_normal, bgl_failure = [], []\n",
                "current_session = []\n",
                "\n",
                "with open(BGL_FILE, 'r', errors='ignore') as f:\n",
                "    for i, line in enumerate(tqdm(f, desc=\"Parsing BGL\")):\n",
                "        if i > 200000: break\n",
                "        line = line.strip()\n",
                "        if not line: continue\n",
                "        \n",
                "        # BGL format: Label ... message\n",
                "        parts = line.split(None, 1)\n",
                "        if len(parts) < 2: continue\n",
                "        \n",
                "        label = parts[0]\n",
                "        is_anomaly = label != '-'\n",
                "        \n",
                "        tid = get_template_id(parts[1] if len(parts) > 1 else line)\n",
                "        current_session.append(tid)\n",
                "        \n",
                "        # Session = 20 events\n",
                "        if len(current_session) >= 20:\n",
                "            if is_anomaly:\n",
                "                bgl_failure.append(current_session.copy())\n",
                "            else:\n",
                "                bgl_normal.append(current_session.copy())\n",
                "            current_session = []\n",
                "\n",
                "print(f\"Templates extracted: {len(template_map)}\")\n",
                "print(f\"BGL Normal: {len(bgl_normal):,}, Failure: {len(bgl_failure):,}\")\n",
                "\n",
                "if len(bgl_normal) == 0:\n",
                "    raise ValueError(\"âŒ No BGL normal sequences parsed!\")\n",
                "\n",
                "# === DATASET & MODEL (vá»›i Ä‘Ãºng VOCAB_SIZE) ===\n",
                "class LogDS(Dataset):\n",
                "    def __init__(self, seqs, vs, ml=CONTEXT_LEN, mr=MASK_RATIO):\n",
                "        self.seqs, self.ml, self.mr, self.vs = seqs, ml, mr, vs\n",
                "    def __len__(self): return len(self.seqs)\n",
                "    def __getitem__(self, i):\n",
                "        s = [min(t+OFF, self.vs-1) for t in self.seqs[i][:self.ml-2]]\n",
                "        tok = [CLS]+s+[SEP]+[PAD]*(self.ml-len(s)-2)\n",
                "        inp, lab = tok.copy(), [-100]*len(tok)\n",
                "        for j in range(1, len(s)+1):\n",
                "            if random.random() < self.mr: lab[j], inp[j] = inp[j], MASK\n",
                "        return {'ids': torch.tensor(inp), 'lab': torch.tensor(lab),\n",
                "                'mask': torch.tensor([1 if t!=PAD else 0 for t in tok])}\n",
                "\n",
                "class LogBERT(nn.Module):\n",
                "    def __init__(self, vs, dm=D_MODEL, nh=N_HEADS, nl=N_LAYERS, ml=CONTEXT_LEN):\n",
                "        super().__init__()\n",
                "        self.tok = nn.Embedding(vs, dm, padding_idx=PAD)\n",
                "        self.pos = nn.Embedding(ml, dm)\n",
                "        self.drop = nn.Dropout(0.1)\n",
                "        el = nn.TransformerEncoderLayer(dm, nh, dm*4, 0.1, 'gelu', batch_first=True)\n",
                "        self.enc = nn.TransformerEncoder(el, nl)\n",
                "        self.head = nn.Linear(dm, vs)\n",
                "        self.register_buffer('ctr', torch.zeros(dm))\n",
                "        self.ci = False\n",
                "    \n",
                "    def forward(self, ids, mask=None):\n",
                "        x = self.tok(ids) + self.pos(torch.arange(ids.size(1), device=ids.device))\n",
                "        h = self.enc(self.drop(x), src_key_padding_mask=(mask==0) if mask is not None else None)\n",
                "        return self.head(h), h[:,0,:]\n",
                "    \n",
                "    def loss(self, lg, lb, cls):\n",
                "        mlm = F.cross_entropy(lg.view(-1, lg.size(-1)), lb.view(-1), ignore_index=-100)\n",
                "        vhm = torch.mean((cls-self.ctr)**2) if self.ci else 0.0\n",
                "        return mlm + LAMBDA_VHM*vhm\n",
                "    \n",
                "    def upd(self, e):\n",
                "        with torch.no_grad():\n",
                "            bc = e.mean(0)\n",
                "            self.ctr = bc if not self.ci else 0.9*self.ctr + 0.1*bc\n",
                "            self.ci = True\n",
                "\n",
                "# Load model\n",
                "model = LogBERT(VOCAB_SIZE).to(device)\n",
                "model.load_state_dict(ckpt)\n",
                "print(f\"âœ… Loaded SimCLR model\")\n",
                "\n",
                "# Reset center for new domain\n",
                "model.ci = False\n",
                "model.ctr = torch.zeros(D_MODEL, device=device)\n",
                "\n",
                "# === TRAIN vá»›i AMP guard ===\n",
                "scaler = torch.amp.GradScaler('cuda') if device.type == 'cuda' else None\n",
                "bgl_loader = DataLoader(LogDS(bgl_normal, vs=VOCAB_SIZE), batch_size=BATCH_SIZE, shuffle=True)\n",
                "bgl_opt = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
                "\n",
                "print(f\"\\nBGL training: {len(bgl_normal):,} sequences, {BGL_EPOCHS} epochs\")\n",
                "\n",
                "best_loss = float('inf')\n",
                "no_improve = 0\n",
                "\n",
                "for ep in range(BGL_EPOCHS):\n",
                "    model.train()\n",
                "    tl = 0\n",
                "    pbar = tqdm(bgl_loader, desc=f\"BGL {ep+1}/{BGL_EPOCHS}\")\n",
                "    for b in pbar:\n",
                "        ids, lab, mask = b['ids'].to(device), b['lab'].to(device), b['mask'].to(device)\n",
                "        bgl_opt.zero_grad()\n",
                "        \n",
                "        if device.type == 'cuda':\n",
                "            with torch.amp.autocast('cuda'):\n",
                "                lg, cls = model(ids, mask)\n",
                "                loss = model.loss(lg, lab, cls)\n",
                "            scaler.scale(loss).backward()\n",
                "            scaler.step(bgl_opt)\n",
                "            scaler.update()\n",
                "        else:\n",
                "            lg, cls = model(ids, mask)\n",
                "            loss = model.loss(lg, lab, cls)\n",
                "            loss.backward()\n",
                "            bgl_opt.step()\n",
                "        \n",
                "        model.upd(cls.detach())\n",
                "        tl += loss.item()\n",
                "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
                "    \n",
                "    avg_loss = tl/len(bgl_loader)\n",
                "    print(f\"BGL Epoch {ep+1}: {avg_loss:.4f}\")\n",
                "    \n",
                "    # Early stopping\n",
                "    if avg_loss < best_loss - 1e-4:\n",
                "        best_loss = avg_loss\n",
                "        no_improve = 0\n",
                "        torch.save(model.state_dict(), f\"{OUTPUT_DIR}/logbert_bgl_best.pt\")\n",
                "    else:\n",
                "        no_improve += 1\n",
                "        if no_improve >= EARLY_STOP:\n",
                "            print(f\"  Early stop at epoch {ep+1}\")\n",
                "            break\n",
                "\n",
                "# === EVALUATE vá»›i MLKP + VHM ===\n",
                "@torch.no_grad()\n",
                "def bert_score_combined(seqs):\n",
                "    \"\"\"Score = MLKP + 0.1*VHM\"\"\"\n",
                "    model.eval()\n",
                "    ld = DataLoader(LogDS(seqs, vs=VOCAB_SIZE, mr=0), batch_size=128, shuffle=False)\n",
                "    scores = []\n",
                "    for b in tqdm(ld, desc=\"Score\"):\n",
                "        ids = b['ids'].to(device)\n",
                "        mask = b['mask'].to(device)\n",
                "        lg, cls = model(ids, mask)\n",
                "        \n",
                "        # MLKP: reconstruction loss\n",
                "        target = ids.clone()\n",
                "        target[ids == PAD] = -100\n",
                "        mlkp = F.cross_entropy(lg.view(-1, lg.size(-1)), target.view(-1), \n",
                "                               ignore_index=-100, reduction='none')\n",
                "        mlkp = mlkp.view(ids.size(0), -1).mean(1)\n",
                "        \n",
                "        # VHM: distance from center\n",
                "        vhm = torch.sum((cls - model.ctr)**2, dim=1)\n",
                "        \n",
                "        # Combined score\n",
                "        score = mlkp + 0.1 * vhm\n",
                "        scores.extend(score.cpu().numpy())\n",
                "    return np.array(scores)\n",
                "\n",
                "def evaluate(n_scores, f_scores):\n",
                "    all_s = np.concatenate([n_scores, f_scores])\n",
                "    all_l = np.concatenate([np.zeros(len(n_scores)), np.ones(len(f_scores))])\n",
                "    auc = roc_auc_score(all_l, all_s)\n",
                "    p, r, th = precision_recall_curve(all_l, all_s)\n",
                "    f1s = 2*p*r/(p+r+1e-10)\n",
                "    idx = np.argmax(f1s)\n",
                "    return {'auc': float(auc), 'f1': float(f1s[idx]), 'p': float(p[idx]), 'r': float(r[idx]), 'th': float(th[idx]) if idx < len(th) else 0}\n",
                "\n",
                "bgl_normal_sc = bert_score_combined(bgl_normal[:5000])\n",
                "bgl_failure_sc = bert_score_combined(bgl_failure[:500] if len(bgl_failure) >= 500 else bgl_failure)\n",
                "bgl_r = evaluate(bgl_normal_sc, bgl_failure_sc)\n",
                "\n",
                "torch.save(model.state_dict(), f\"{OUTPUT_DIR}/logbert_bgl.pt\")\n",
                "\n",
                "# Save results\n",
                "with open(f\"{OUTPUT_DIR}/bgl_results.json\", 'w') as f:\n",
                "    json.dump({'bgl': bgl_r, 'templates': len(template_map)}, f, indent=2)\n",
                "\n",
                "print(\"\\n\" + \"=\"*70)\n",
                "print(\"ðŸŽ¯ BGL RESULTS\")\n",
                "print(\"=\"*70)\n",
                "print(f\"BGL: AUC={bgl_r['auc']:.4f}, F1={bgl_r['f1']:.4f}, P={bgl_r['p']:.4f}, R={bgl_r['r']:.4f}\")\n",
                "print(f\"Threshold: {bgl_r['th']:.4f}\")\n",
                "print(f\"\\nâœ“ Saved: {OUTPUT_DIR}/logbert_bgl.pt, bgl_results.json\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
