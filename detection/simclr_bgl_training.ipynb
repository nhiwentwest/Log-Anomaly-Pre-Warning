{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üéØ Phase 2: SimCLR Contrastive Fine-tuning + BGL Cross-Domain\n",
                "\n",
                "**Prerequisites:** Run `complete_multi_layer.ipynb` first to get:\n",
                "- `output/logbert_full.pt` - Pre-trained model\n",
                "- `output/v2_sequences.jsonl` - HDFS sequences\n",
                "\n",
                "**This notebook:**\n",
                "1. SimCLR contrastive fine-tuning\n",
                "2. BGL cross-domain adaptation\n",
                "3. Compare results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#=============================================================================\n",
                "# CELL 1: SETUP\n",
                "#=============================================================================\n",
                "import os, gc, json, random, copy\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "from collections import defaultdict, Counter\n",
                "from tqdm import tqdm\n",
                "\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "from sklearn.metrics import precision_recall_curve, roc_auc_score\n",
                "\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f\"Device: {device}\")\n",
                "\n",
                "# Config\n",
                "CONTEXT_LEN = 128\n",
                "D_MODEL, N_HEADS, N_LAYERS = 256, 8, 4\n",
                "BATCH_SIZE = 64\n",
                "SIMCLR_EPOCHS = 10\n",
                "BGL_EPOCHS = 15\n",
                "LR = 1e-5  # Lower LR for fine-tuning\n",
                "TEMPERATURE = 0.1  # SimCLR temperature\n",
                "SEED = 42\n",
                "\n",
                "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
                "\n",
                "OUTPUT_DIR = \"output\"\n",
                "os.makedirs(OUTPUT_DIR, exist_ok=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#=============================================================================\n",
                "# CELL 2: LOAD PRE-TRAINED MODEL\n",
                "#=============================================================================\n",
                "print(\"=\"*60)\n",
                "print(\"LOADING PRE-TRAINED MODEL\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "PAD, CLS, MASK, SEP, OFF = 0, 1, 2, 3, 4\n",
                "\n",
                "class LogBERT(nn.Module):\n",
                "    def __init__(self, vs, dm=D_MODEL, nh=N_HEADS, nl=N_LAYERS, ml=CONTEXT_LEN, lv=0.5):\n",
                "        super().__init__()\n",
                "        self.lv = lv\n",
                "        self.tok = nn.Embedding(vs, dm, padding_idx=PAD)\n",
                "        self.pos = nn.Embedding(ml, dm)\n",
                "        self.drop = nn.Dropout(0.1)\n",
                "        el = nn.TransformerEncoderLayer(dm, nh, dm*4, 0.1, 'gelu', batch_first=True)\n",
                "        self.enc = nn.TransformerEncoder(el, nl)\n",
                "        self.head = nn.Linear(dm, vs)\n",
                "        self.register_buffer('ctr', torch.zeros(dm))\n",
                "        self.ci = False\n",
                "    \n",
                "    def forward(self, ids, mask=None):\n",
                "        B, L = ids.shape\n",
                "        x = self.tok(ids) + self.pos(torch.arange(L, device=ids.device).unsqueeze(0))\n",
                "        h = self.enc(self.drop(x), src_key_padding_mask=(mask==0) if mask is not None else None)\n",
                "        return self.head(h), h[:,0,:]\n",
                "    \n",
                "    def get_embedding(self, ids, mask=None):\n",
                "        \"\"\"Get CLS embedding for contrastive learning.\"\"\"\n",
                "        _, cls = self.forward(ids, mask)\n",
                "        return cls\n",
                "\n",
                "# Load pre-trained model\n",
                "model_path = os.path.join(OUTPUT_DIR, \"logbert_full.pt\")\n",
                "if os.path.exists(model_path):\n",
                "    # Get vocab size from saved model\n",
                "    state = torch.load(model_path, map_location='cpu')\n",
                "    VOCAB_SIZE = state['tok.weight'].shape[0]\n",
                "    print(f\"Vocab size: {VOCAB_SIZE}\")\n",
                "    \n",
                "    model = LogBERT(VOCAB_SIZE).to(device)\n",
                "    model.load_state_dict(state)\n",
                "    print(\"‚úì Loaded pre-trained model\")\n",
                "else:\n",
                "    print(f\"ERROR: {model_path} not found!\")\n",
                "    print(\"Run complete_multi_layer.ipynb first.\")\n",
                "    VOCAB_SIZE = 2048  # Fallback"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# üîÑ PHASE 1: SimCLR Contrastive Learning"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#=============================================================================\n",
                "# CELL 3: SIMCLR AUGMENTATION\n",
                "#=============================================================================\n",
                "\n",
                "def augment_sequence(seq, aug_type='mask', mask_ratio=0.15):\n",
                "    \"\"\"\n",
                "    Augment sequence for contrastive learning.\n",
                "    \n",
                "    Augmentation types:\n",
                "    - mask: Random masking\n",
                "    - drop: Random token dropping\n",
                "    - shuffle: Local shuffling (within window)\n",
                "    - crop: Random cropping\n",
                "    \"\"\"\n",
                "    seq = list(seq)  # Copy\n",
                "    \n",
                "    if aug_type == 'mask':\n",
                "        # Replace random tokens with MASK\n",
                "        for i in range(len(seq)):\n",
                "            if random.random() < mask_ratio:\n",
                "                seq[i] = MASK\n",
                "    \n",
                "    elif aug_type == 'drop':\n",
                "        # Drop random tokens\n",
                "        seq = [t for t in seq if random.random() > mask_ratio]\n",
                "        if len(seq) == 0:\n",
                "            seq = [SEP]  # Keep at least one\n",
                "    \n",
                "    elif aug_type == 'shuffle':\n",
                "        # Local shuffling within windows of size 3\n",
                "        window = 3\n",
                "        for i in range(0, len(seq) - window + 1, window):\n",
                "            chunk = seq[i:i+window]\n",
                "            random.shuffle(chunk)\n",
                "            seq[i:i+window] = chunk\n",
                "    \n",
                "    elif aug_type == 'crop':\n",
                "        # Random crop (keep 70-90%)\n",
                "        keep_ratio = random.uniform(0.7, 0.9)\n",
                "        new_len = max(1, int(len(seq) * keep_ratio))\n",
                "        start = random.randint(0, len(seq) - new_len)\n",
                "        seq = seq[start:start + new_len]\n",
                "    \n",
                "    return seq\n",
                "\n",
                "class ContrastiveDataset(Dataset):\n",
                "    \"\"\"Dataset for SimCLR contrastive learning.\"\"\"\n",
                "    \n",
                "    def __init__(self, sequences, max_len=CONTEXT_LEN):\n",
                "        self.seqs = sequences\n",
                "        self.max_len = max_len\n",
                "        self.aug_types = ['mask', 'drop', 'shuffle', 'crop']\n",
                "    \n",
                "    def __len__(self):\n",
                "        return len(self.seqs)\n",
                "    \n",
                "    def _tokenize(self, seq):\n",
                "        s = [t + OFF for t in seq[:self.max_len - 2]]\n",
                "        tok = [CLS] + s + [SEP] + [PAD] * (self.max_len - len(s) - 2)\n",
                "        mask = [1 if t != PAD else 0 for t in tok]\n",
                "        return torch.tensor(tok), torch.tensor(mask)\n",
                "    \n",
                "    def __getitem__(self, idx):\n",
                "        seq = self.seqs[idx]\n",
                "        \n",
                "        # Get two different augmented views\n",
                "        aug1 = random.choice(self.aug_types)\n",
                "        aug2 = random.choice(self.aug_types)\n",
                "        \n",
                "        seq1 = augment_sequence(seq, aug1)\n",
                "        seq2 = augment_sequence(seq, aug2)\n",
                "        \n",
                "        ids1, mask1 = self._tokenize(seq1)\n",
                "        ids2, mask2 = self._tokenize(seq2)\n",
                "        \n",
                "        return {\n",
                "            'ids1': ids1, 'mask1': mask1,\n",
                "            'ids2': ids2, 'mask2': mask2\n",
                "        }\n",
                "\n",
                "print(\"ContrastiveDataset defined.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#=============================================================================\n",
                "# CELL 4: SIMCLR LOSS (NT-Xent)\n",
                "#=============================================================================\n",
                "\n",
                "class ProjectionHead(nn.Module):\n",
                "    \"\"\"MLP projection head for SimCLR.\"\"\"\n",
                "    def __init__(self, input_dim=D_MODEL, hidden_dim=512, output_dim=128):\n",
                "        super().__init__()\n",
                "        self.net = nn.Sequential(\n",
                "            nn.Linear(input_dim, hidden_dim),\n",
                "            nn.ReLU(),\n",
                "            nn.Linear(hidden_dim, output_dim)\n",
                "        )\n",
                "    \n",
                "    def forward(self, x):\n",
                "        return F.normalize(self.net(x), dim=1)\n",
                "\n",
                "def nt_xent_loss(z1, z2, temperature=TEMPERATURE):\n",
                "    \"\"\"\n",
                "    NT-Xent (Normalized Temperature-scaled Cross Entropy) loss.\n",
                "    SimCLR contrastive loss.\n",
                "    \"\"\"\n",
                "    B = z1.shape[0]\n",
                "    z = torch.cat([z1, z2], dim=0)  # 2B x D\n",
                "    \n",
                "    # Similarity matrix\n",
                "    sim = torch.mm(z, z.t()) / temperature  # 2B x 2B\n",
                "    \n",
                "    # Mask out self-similarity\n",
                "    mask = torch.eye(2 * B, device=z.device).bool()\n",
                "    sim.masked_fill_(mask, float('-inf'))\n",
                "    \n",
                "    # Positive pairs: (i, i+B) and (i+B, i)\n",
                "    pos_mask = torch.zeros(2 * B, 2 * B, device=z.device)\n",
                "    for i in range(B):\n",
                "        pos_mask[i, i + B] = 1\n",
                "        pos_mask[i + B, i] = 1\n",
                "    \n",
                "    # Labels for cross-entropy\n",
                "    labels = torch.cat([torch.arange(B, 2*B), torch.arange(B)], dim=0).to(z.device)\n",
                "    \n",
                "    loss = F.cross_entropy(sim, labels)\n",
                "    return loss\n",
                "\n",
                "print(\"SimCLR loss defined.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#=============================================================================\n",
                "# CELL 5: LOAD HDFS SEQUENCES FOR SIMCLR\n",
                "#=============================================================================\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"LOADING HDFS SEQUENCES\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "# Load from JSONL\n",
                "hdfs_seqs = []\n",
                "jsonl_path = os.path.join(OUTPUT_DIR, \"v2_sequences.jsonl\")\n",
                "\n",
                "if os.path.exists(jsonl_path):\n",
                "    with open(jsonl_path, 'r') as f:\n",
                "        for line in f:\n",
                "            data = json.loads(line)\n",
                "            hdfs_seqs.append(data['seq'])\n",
                "    print(f\"Loaded {len(hdfs_seqs):,} HDFS sequences\")\n",
                "else:\n",
                "    print(\"v2_sequences.jsonl not found, creating dummy data...\")\n",
                "    # Create some dummy sequences for testing\n",
                "    for _ in range(10000):\n",
                "        seq_len = random.randint(5, 50)\n",
                "        hdfs_seqs.append([random.randint(0, 100) for _ in range(seq_len)])\n",
                "\n",
                "print(f\"Avg sequence length: {np.mean([len(s) for s in hdfs_seqs]):.1f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#=============================================================================\n",
                "# CELL 6: SIMCLR FINE-TUNING\n",
                "#=============================================================================\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"SIMCLR CONTRASTIVE FINE-TUNING\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "# Create projection head\n",
                "projection = ProjectionHead(D_MODEL).to(device)\n",
                "\n",
                "# Prepare data\n",
                "contrast_ds = ContrastiveDataset(hdfs_seqs)\n",
                "contrast_loader = DataLoader(contrast_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, drop_last=True)\n",
                "\n",
                "# Optimizer for both model and projection head\n",
                "optimizer = torch.optim.AdamW(\n",
                "    list(model.parameters()) + list(projection.parameters()),\n",
                "    lr=LR\n",
                ")\n",
                "\n",
                "print(f\"Training with SimCLR for {SIMCLR_EPOCHS} epochs\")\n",
                "print(f\"Temperature: {TEMPERATURE}\")\n",
                "\n",
                "for epoch in range(SIMCLR_EPOCHS):\n",
                "    model.train()\n",
                "    projection.train()\n",
                "    total_loss = 0\n",
                "    \n",
                "    pbar = tqdm(contrast_loader, desc=f\"Epoch {epoch+1}/{SIMCLR_EPOCHS}\")\n",
                "    for batch in pbar:\n",
                "        ids1 = batch['ids1'].to(device)\n",
                "        mask1 = batch['mask1'].to(device)\n",
                "        ids2 = batch['ids2'].to(device)\n",
                "        mask2 = batch['mask2'].to(device)\n",
                "        \n",
                "        optimizer.zero_grad()\n",
                "        \n",
                "        # Get embeddings\n",
                "        emb1 = model.get_embedding(ids1, mask1)\n",
                "        emb2 = model.get_embedding(ids2, mask2)\n",
                "        \n",
                "        # Project\n",
                "        z1 = projection(emb1)\n",
                "        z2 = projection(emb2)\n",
                "        \n",
                "        # Contrastive loss\n",
                "        loss = nt_xent_loss(z1, z2, TEMPERATURE)\n",
                "        \n",
                "        loss.backward()\n",
                "        optimizer.step()\n",
                "        \n",
                "        total_loss += loss.item()\n",
                "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
                "    \n",
                "    avg_loss = total_loss / len(contrast_loader)\n",
                "    print(f\"Epoch {epoch+1}: SimCLR Loss = {avg_loss:.4f}\")\n",
                "\n",
                "# Save SimCLR fine-tuned model\n",
                "torch.save(model.state_dict(), os.path.join(OUTPUT_DIR, \"logbert_simclr.pt\"))\n",
                "print(\"\\n‚úì SimCLR fine-tuning complete!\")\n",
                "print(\"‚úì Saved: logbert_simclr.pt\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# üåê PHASE 2: BGL Cross-Domain Adaptation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#=============================================================================\n",
                "# CELL 7: LOAD BGL DATA\n",
                "#=============================================================================\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"BGL CROSS-DOMAIN ADAPTATION\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "# Possible BGL paths\n",
                "BGL_PATHS = [\n",
                "    \"/teamspace/studios/this_studio/content/LogHub_BGL\",\n",
                "    \"/teamspace/studios/this_studio/LogHub_BGL\",\n",
                "    \"/content/LogHub_BGL\",\n",
                "    \"./LogHub_BGL\",\n",
                "]\n",
                "\n",
                "BGL_PATH = next((p for p in BGL_PATHS if os.path.exists(p)), None)\n",
                "\n",
                "bgl_normal, bgl_failure = [], []\n",
                "\n",
                "if BGL_PATH:\n",
                "    print(f\"BGL found at: {BGL_PATH}\")\n",
                "    \n",
                "    # Try different file formats\n",
                "    bgl_file = os.path.join(BGL_PATH, \"BGL.log_structured.csv\")\n",
                "    if not os.path.exists(bgl_file):\n",
                "        bgl_file = os.path.join(BGL_PATH, \"BGL_sequence.csv\")\n",
                "    \n",
                "    if os.path.exists(bgl_file):\n",
                "        print(f\"Loading {bgl_file}...\")\n",
                "        df = pd.read_csv(bgl_file, nrows=100000)  # Limit for speed\n",
                "        print(f\"Columns: {df.columns.tolist()}\")\n",
                "        \n",
                "        # Group by BlockId or SessionWindow\n",
                "        if 'BlockId' in df.columns:\n",
                "            group_col = 'BlockId'\n",
                "        elif 'SessionWindow' in df.columns:\n",
                "            group_col = 'SessionWindow'\n",
                "        else:\n",
                "            # Create sessions by time window\n",
                "            df['Session'] = df.index // 100\n",
                "            group_col = 'Session'\n",
                "        \n",
                "        # Get EventId column\n",
                "        event_col = 'EventId' if 'EventId' in df.columns else 'EventTemplate'\n",
                "        label_col = 'Label' if 'Label' in df.columns else None\n",
                "        \n",
                "        # Build sequences\n",
                "        for name, group in df.groupby(group_col):\n",
                "            events = group[event_col].tolist()\n",
                "            # Convert to IDs if strings\n",
                "            if isinstance(events[0], str):\n",
                "                events = [hash(e) % 1000 for e in events]\n",
                "            \n",
                "            if label_col and 'Anomaly' in str(group[label_col].iloc[0]):\n",
                "                bgl_failure.append(events)\n",
                "            else:\n",
                "                bgl_normal.append(events)\n",
                "        \n",
                "        print(f\"BGL Normal: {len(bgl_normal):,}\")\n",
                "        print(f\"BGL Failure: {len(bgl_failure):,}\")\n",
                "    else:\n",
                "        print(f\"BGL file not found at {bgl_file}\")\n",
                "else:\n",
                "    print(\"BGL dataset not found. Creating synthetic data for demo...\")\n",
                "    # Create synthetic BGL-like data\n",
                "    for _ in range(5000):\n",
                "        seq_len = random.randint(10, 100)\n",
                "        bgl_normal.append([random.randint(0, 500) for _ in range(seq_len)])\n",
                "    for _ in range(500):\n",
                "        seq_len = random.randint(10, 100)\n",
                "        bgl_failure.append([random.randint(0, 500) for _ in range(seq_len)])\n",
                "    print(f\"Created synthetic BGL data: {len(bgl_normal)} normal, {len(bgl_failure)} failure\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#=============================================================================\n",
                "# CELL 8: BGL DATASET\n",
                "#=============================================================================\n",
                "\n",
                "class BGLDataset(Dataset):\n",
                "    def __init__(self, sequences, max_len=CONTEXT_LEN, mask_ratio=0.15):\n",
                "        self.seqs = sequences\n",
                "        self.max_len = max_len\n",
                "        self.mr = mask_ratio\n",
                "    \n",
                "    def __len__(self):\n",
                "        return len(self.seqs)\n",
                "    \n",
                "    def __getitem__(self, idx):\n",
                "        seq = self.seqs[idx][:self.max_len - 2]\n",
                "        # Map to vocab range\n",
                "        seq = [min(t + OFF, VOCAB_SIZE - 1) for t in seq]\n",
                "        tok = [CLS] + seq + [SEP] + [PAD] * (self.max_len - len(seq) - 2)\n",
                "        \n",
                "        inp, lab = tok.copy(), [-100] * len(tok)\n",
                "        for i in range(1, len(seq) + 1):\n",
                "            if random.random() < self.mr:\n",
                "                lab[i], inp[i] = inp[i], MASK\n",
                "        \n",
                "        return {\n",
                "            'input_ids': torch.tensor(inp),\n",
                "            'labels': torch.tensor(lab),\n",
                "            'attention_mask': torch.tensor([1 if t != PAD else 0 for t in tok])\n",
                "        }\n",
                "\n",
                "print(f\"BGLDataset defined. Vocab size: {VOCAB_SIZE}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#=============================================================================\n",
                "# CELL 9: FINE-TUNE ON BGL (Domain Adaptation)\n",
                "#=============================================================================\n",
                "print(\"\\nFine-tuning on BGL...\")\n",
                "\n",
                "# Reset model center for new domain\n",
                "model.ci = False\n",
                "model.ctr = torch.zeros(D_MODEL, device=device)\n",
                "\n",
                "bgl_loader = DataLoader(\n",
                "    BGLDataset(bgl_normal),\n",
                "    batch_size=BATCH_SIZE,\n",
                "    shuffle=True,\n",
                "    num_workers=2\n",
                ")\n",
                "\n",
                "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
                "scaler = torch.cuda.amp.GradScaler()\n",
                "\n",
                "print(f\"BGL training: {len(bgl_normal):,} normal sequences, {BGL_EPOCHS} epochs\")\n",
                "\n",
                "for epoch in range(BGL_EPOCHS):\n",
                "    model.train()\n",
                "    total_loss = 0\n",
                "    pbar = tqdm(bgl_loader, desc=f\"BGL Ep {epoch+1}/{BGL_EPOCHS}\")\n",
                "    \n",
                "    for batch in pbar:\n",
                "        ids = batch['input_ids'].to(device)\n",
                "        lab = batch['labels'].to(device)\n",
                "        attn = batch['attention_mask'].to(device)\n",
                "        \n",
                "        optimizer.zero_grad()\n",
                "        \n",
                "        with torch.cuda.amp.autocast():\n",
                "            logits, cls = model(ids, attn)\n",
                "            mlm_loss = F.cross_entropy(\n",
                "                logits.view(-1, logits.size(-1)),\n",
                "                lab.view(-1),\n",
                "                ignore_index=-100\n",
                "            )\n",
                "            # VHM loss for new domain\n",
                "            if model.ci:\n",
                "                vhm_loss = torch.mean((cls - model.ctr) ** 2)\n",
                "            else:\n",
                "                vhm_loss = 0.0\n",
                "            loss = mlm_loss + 0.5 * vhm_loss\n",
                "        \n",
                "        scaler.scale(loss).backward()\n",
                "        scaler.step(optimizer)\n",
                "        scaler.update()\n",
                "        \n",
                "        # Update center\n",
                "        with torch.no_grad():\n",
                "            bc = cls.mean(0)\n",
                "            model.ctr = bc if not model.ci else 0.9 * model.ctr + 0.1 * bc\n",
                "            model.ci = True\n",
                "        \n",
                "        total_loss += loss.item()\n",
                "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
                "    \n",
                "    print(f\"BGL Epoch {epoch+1}: Loss = {total_loss/len(bgl_loader):.4f}\")\n",
                "\n",
                "# Save BGL-adapted model\n",
                "torch.save(model.state_dict(), os.path.join(OUTPUT_DIR, \"logbert_bgl.pt\"))\n",
                "print(\"\\n‚úì BGL fine-tuning complete!\")\n",
                "print(\"‚úì Saved: logbert_bgl.pt\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#=============================================================================\n",
                "# CELL 10: EVALUATE ON BGL\n",
                "#=============================================================================\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"EVALUATING ON BGL\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "@torch.no_grad()\n",
                "def compute_scores(model, sequences, batch_size=128):\n",
                "    model.eval()\n",
                "    loader = DataLoader(BGLDataset(sequences, mask_ratio=0), batch_size=batch_size, shuffle=False)\n",
                "    scores = []\n",
                "    for batch in tqdm(loader, desc=\"Scoring\"):\n",
                "        _, cls = model(batch['input_ids'].to(device), batch['attention_mask'].to(device))\n",
                "        dist = torch.sum((cls - model.ctr) ** 2, dim=1)\n",
                "        scores.extend(dist.cpu().numpy())\n",
                "    return np.array(scores)\n",
                "\n",
                "# Compute scores\n",
                "bgl_normal_scores = compute_scores(model, bgl_normal)\n",
                "bgl_failure_scores = compute_scores(model, bgl_failure)\n",
                "\n",
                "# Evaluate\n",
                "all_scores = np.concatenate([bgl_normal_scores, bgl_failure_scores])\n",
                "all_labels = np.concatenate([np.zeros(len(bgl_normal_scores)), np.ones(len(bgl_failure_scores))])\n",
                "\n",
                "auc = roc_auc_score(all_labels, all_scores)\n",
                "prec, rec, _ = precision_recall_curve(all_labels, all_scores)\n",
                "f1s = 2 * prec * rec / (prec + rec + 1e-10)\n",
                "best_f1 = np.max(f1s)\n",
                "best_idx = np.argmax(f1s)\n",
                "\n",
                "print(f\"\\n=== BGL Results ===\")\n",
                "print(f\"AUC-ROC: {auc:.4f}\")\n",
                "print(f\"Best F1: {best_f1:.4f}\")\n",
                "print(f\"Precision: {prec[best_idx]:.4f}\")\n",
                "print(f\"Recall: {rec[best_idx]:.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#=============================================================================\n",
                "# CELL 11: FINAL SUMMARY\n",
                "#=============================================================================\n",
                "print(\"\\n\" + \"=\"*70)\n",
                "print(\"üéØ SUMMARY\")\n",
                "print(\"=\"*70)\n",
                "\n",
                "print(\"\\n=== Models Saved ===\")\n",
                "print(f\"‚úì logbert_full.pt    - Pre-trained on HDFS V1+V2+V3\")\n",
                "print(f\"‚úì logbert_simclr.pt  - SimCLR contrastive fine-tuned\")\n",
                "print(f\"‚úì logbert_bgl.pt     - BGL domain adapted\")\n",
                "\n",
                "print(\"\\n=== BGL Cross-Domain Performance ===\")\n",
                "print(f\"AUC: {auc:.4f}\")\n",
                "print(f\"F1:  {best_f1:.4f}\")\n",
                "\n",
                "# Save results\n",
                "results = {\n",
                "    'simclr': {\n",
                "        'epochs': SIMCLR_EPOCHS,\n",
                "        'temperature': TEMPERATURE\n",
                "    },\n",
                "    'bgl': {\n",
                "        'epochs': BGL_EPOCHS,\n",
                "        'normal': len(bgl_normal),\n",
                "        'failure': len(bgl_failure),\n",
                "        'auc': float(auc),\n",
                "        'f1': float(best_f1),\n",
                "        'precision': float(prec[best_idx]),\n",
                "        'recall': float(rec[best_idx])\n",
                "    }\n",
                "}\n",
                "\n",
                "with open(os.path.join(OUTPUT_DIR, \"simclr_bgl_results.json\"), 'w') as f:\n",
                "    json.dump(results, f, indent=2)\n",
                "\n",
                "print(\"\\n‚úì Results saved to simclr_bgl_results.json\")\n",
                "print(\"\\n\" + \"=\"*70)\n",
                "print(\"üéâ COMPLETE!\")\n",
                "print(\"=\"*70)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}