{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# HDFS TRUE Pre-Warning (Curriculum Learning)\n",
                "\n",
                "**Stage 1:** Detection + Pre-warning \u2192 Learn patterns\n",
                "**Stage 2:** TRUE Pre-warning \u2192 Fine-tune"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os, json, random\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "from tqdm import tqdm\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "from sklearn.metrics import roc_auc_score, precision_recall_curve\n",
                "\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f\"Device: {device}\")\n",
                "\n",
                "BASE = \"/teamspace/studios/this_studio\"\n",
                "V1_PATH = f\"{BASE}/content/LogHub_HDFS/HDFS_v1/preprocessed/Event_traces.csv\"\n",
                "CKPT_DIR = f\"{BASE}/checkpoints\"\n",
                "OUTPUT_DIR = f\"{BASE}/output-hdfsv1ew1\"  # New output dir\n",
                "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
                "\n",
                "CONTEXT_LEN, D_MODEL, N_HEADS, N_LAYERS = 128, 256, 8, 4\n",
                "PAD, CLS, MASK, SEP, OFF = 0, 1, 2, 3, 4\n",
                "BATCH_SIZE, SEED = 64, 42\n",
                "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
                "\n",
                "# IMPROVED: LOOKAHEAD_BLOCKS instead of PRE_RATIO\n",
                "LOOKAHEAD_BLOCKS = 15  # Label=1 if anomaly in next N blocks\n",
                "print(f\"Using LOOKAHEAD_BLOCKS = {LOOKAHEAD_BLOCKS}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# LOAD DATA\n",
                "df = pd.read_csv(V1_PATH)\n",
                "v1_normal, v1_failure = [], []\n",
                "for _, row in tqdm(df.iterrows(), total=len(df), desc=\"V1\"):\n",
                "    features = str(row.get('Features', '[]')).strip('[]\"')\n",
                "    events = [int(e.strip().strip(\"'\")[1:]) for e in features.split(',') if e.strip().startswith('E')]\n",
                "    if len(events) >= 6:\n",
                "        (v1_normal if row['Label'] == 'Success' else v1_failure).append(events)\n",
                "\n",
                "print(f\"Normal: {len(v1_normal):,}, Failure: {len(v1_failure):,}\")\n",
                "\n",
                "def seed_split(seqs, r=0.2):\n",
                "    rng = np.random.RandomState(SEED)\n",
                "    idx = rng.permutation(len(seqs))\n",
                "    s = int(len(seqs)*(1-r))\n",
                "    return [seqs[i] for i in idx[:s]], [seqs[i] for i in idx[s:]]\n",
                "\n",
                "normal_train, normal_test = seed_split(v1_normal)\n",
                "failure_train, failure_test = seed_split(v1_failure)\n",
                "print(f\"TRAIN: {len(normal_train):,} N, {len(failure_train):,} F\")\n",
                "print(f\"TEST: {len(normal_test):,} N, {len(failure_test):,} F\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# STAGE 1: Detection + Pre-warning (LOOKAHEAD_BLOCKS approach)\n",
                "def create_prefixes_stage1(traces, is_failure=False, lookahead=LOOKAHEAD_BLOCKS, step=3, min_len=6, max_per=15):\n",
                "    \"\"\"Label=1 if this prefix is within LOOKAHEAD of trace end (failure).\"\"\"\n",
                "    samples = []\n",
                "    for trace in traces:\n",
                "        if len(trace) < min_len: continue\n",
                "        prefixes = []\n",
                "        for k in range(min_len, len(trace)+1, step):\n",
                "            remaining = len(trace) - k\n",
                "            # For failure traces: label=1 if close to end (within lookahead)\n",
                "            if is_failure:\n",
                "                ew = 1 if remaining <= lookahead else 0\n",
                "            else:\n",
                "                ew = 0\n",
                "            prefixes.append((trace[:k], ew, remaining))  # Include remaining for position encoding\n",
                "        if len(prefixes) > max_per:\n",
                "            # Keep mix of pos and neg\n",
                "            pos = [p for p in prefixes if p[1]==1]\n",
                "            neg = [p for p in prefixes if p[1]==0]\n",
                "            if len(pos) > max_per//2:\n",
                "                pos = random.sample(pos, max_per//2)\n",
                "            neg = random.sample(neg, min(len(neg), max_per - len(pos)))\n",
                "            prefixes = pos + neg\n",
                "        samples.extend(prefixes)\n",
                "    return samples\n",
                "\n",
                "print(\"Stage 1 prefixes (LOOKAHEAD approach)...\")\n",
                "s1_normal = create_prefixes_stage1(normal_train, is_failure=False)\n",
                "s1_failure = create_prefixes_stage1(failure_train, is_failure=True)\n",
                "s1_pos = sum(1 for p in s1_failure if p[1]==1)\n",
                "print(f\"Stage 1: normal={len(s1_normal):,}, failure={len(s1_failure):,}, pos={s1_pos:,}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# STAGE 2: TRUE Pre-warning (skip last LOOKAHEAD blocks - already in danger zone)\n",
                "def create_prefixes_stage2(traces, lookahead=LOOKAHEAD_BLOCKS, step=3, min_len=6, max_per=15):\n",
                "    \"\"\"TRUE pre-warning: label=1 if prefix BEFORE danger zone but failure coming.\"\"\"\n",
                "    samples = []\n",
                "    for trace in traces:\n",
                "        if len(trace) < min_len + lookahead: continue  # Need enough for lookahead\n",
                "        prefixes = []\n",
                "        for k in range(min_len, len(trace)+1, step):\n",
                "            remaining = len(trace) - k\n",
                "            if remaining <= lookahead:\n",
                "                continue  # SKIP: already in danger zone (detection, not pre-warning)\n",
                "            # remaining > lookahead: true pre-warning opportunity\n",
                "            ew = 1  # Failure is coming\n",
                "            prefixes.append((trace[:k], ew, remaining))\n",
                "        if len(prefixes) > max_per:\n",
                "            prefixes = random.sample(prefixes, max_per)\n",
                "        samples.extend(prefixes)\n",
                "    return samples\n",
                "\n",
                "print(\"Stage 2 prefixes (TRUE pre-warning)...\")\n",
                "s2_normal = [(trace[:k], 0, len(trace)-k) for trace in normal_train for k in range(6, len(trace)+1, 3)][:len(normal_train)*15]\n",
                "s2_failure = create_prefixes_stage2(failure_train)\n",
                "s2_pos = sum(1 for p in s2_failure if p[1]==1)\n",
                "print(f\"Stage 2: normal={len(s2_normal):,}, failure pos={s2_pos:,}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# DATASET\n",
                "class EWDataset(Dataset):\n",
                "    def __init__(self, normal, failure, vs, ratio=5):\n",
                "        self.pos = [p for p in failure if p[1]==1]\n",
                "        self.neg = normal + [p for p in failure if p[1]==0]\n",
                "        self.vs, self.ratio = vs, ratio\n",
                "        self.n = len(self.pos)*(1+ratio) if self.pos else len(self.neg)\n",
                "    def __len__(self): return self.n\n",
                "    def __getitem__(self, idx):\n",
                "        if self.pos and idx%(self.ratio+1)==0:\n",
                "            prefix, ew = random.choice(self.pos)\n",
                "        else:\n",
                "            prefix, ew = random.choice(self.neg) if self.neg else random.choice(self.pos)\n",
                "        s = [min(t+OFF, self.vs-1) for t in prefix[:CONTEXT_LEN-2]]\n",
                "        tok = [CLS]+s+[SEP]+[PAD]*(CONTEXT_LEN-len(s)-2)\n",
                "        masked, mlm = tok.copy(), [-100]*len(tok)\n",
                "        if s:\n",
                "            for p in np.random.choice(len(s), min(max(1,int(len(s)*0.15)),len(s)), replace=False)+1:\n",
                "                mlm[p] = masked[p]; masked[p] = MASK\n",
                "        return {'ids': torch.tensor(masked), 'mask': torch.tensor([1 if t!=PAD else 0 for t in masked]),\n",
                "                'ew': torch.tensor(ew, dtype=torch.float), 'mlm': torch.tensor(mlm)}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# MODEL\n",
                "class LogBERTEW(nn.Module):\n",
                "    def __init__(self, vs):\n",
                "        super().__init__()\n",
                "        self.tok = nn.Embedding(vs, D_MODEL, padding_idx=PAD)\n",
                "        self.pos = nn.Embedding(CONTEXT_LEN, D_MODEL)\n",
                "        self.drop = nn.Dropout(0.1)\n",
                "        self.enc = nn.TransformerEncoder(nn.TransformerEncoderLayer(D_MODEL, N_HEADS, D_MODEL*4, 0.1, 'gelu', batch_first=True), N_LAYERS)\n",
                "        self.head = nn.Linear(D_MODEL, vs)\n",
                "        self.ew_head = nn.Linear(D_MODEL, 1)\n",
                "        self.register_buffer('ctr', torch.zeros(D_MODEL)); self.ci = False\n",
                "    def forward(self, ids, mask=None):\n",
                "        x = self.tok(ids) + self.pos(torch.arange(ids.size(1), device=ids.device))\n",
                "        h = self.enc(self.drop(x), src_key_padding_mask=(mask==0) if mask is not None else None)\n",
                "        return self.head(h), h[:,0,:], self.ew_head(h[:,0,:]).squeeze(-1)\n",
                "    def upd(self, e):\n",
                "        with torch.no_grad(): bc = e.mean(0); self.ctr = bc if not self.ci else 0.9*self.ctr+0.1*bc; self.ci = True\n",
                "\n",
                "ckpt = torch.load(f\"{CKPT_DIR}/logbert_ep11.pt\", map_location=device)\n",
                "VOCAB_SIZE = ckpt['model_state_dict']['tok.weight'].shape[0]\n",
                "model = LogBERTEW(VOCAB_SIZE).to(device)\n",
                "model.load_state_dict(ckpt['model_state_dict'], strict=False)\n",
                "model.ctr = ckpt['center'].to(device); model.ci = True\n",
                "print(f\"Model loaded: VOCAB={VOCAB_SIZE}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# STAGE 1: Detection + Pre-warning (LOOKAHEAD_BLOCKS approach)\n",
                "def create_prefixes_stage1(traces, is_failure=False, lookahead=LOOKAHEAD_BLOCKS, step=3, min_len=6, max_per=15):\n",
                "    \"\"\"Label=1 if this prefix is within LOOKAHEAD of trace end (failure).\"\"\"\n",
                "    samples = []\n",
                "    for trace in traces:\n",
                "        if len(trace) < min_len: continue\n",
                "        prefixes = []\n",
                "        for k in range(min_len, len(trace)+1, step):\n",
                "            remaining = len(trace) - k\n",
                "            # For failure traces: label=1 if close to end (within lookahead)\n",
                "            if is_failure:\n",
                "                ew = 1 if remaining <= lookahead else 0\n",
                "            else:\n",
                "                ew = 0\n",
                "            prefixes.append((trace[:k], ew, remaining))  # Include remaining for position encoding\n",
                "        if len(prefixes) > max_per:\n",
                "            # Keep mix of pos and neg\n",
                "            pos = [p for p in prefixes if p[1]==1]\n",
                "            neg = [p for p in prefixes if p[1]==0]\n",
                "            if len(pos) > max_per//2:\n",
                "                pos = random.sample(pos, max_per//2)\n",
                "            neg = random.sample(neg, min(len(neg), max_per - len(pos)))\n",
                "            prefixes = pos + neg\n",
                "        samples.extend(prefixes)\n",
                "    return samples\n",
                "\n",
                "print(\"Stage 1 prefixes (LOOKAHEAD approach)...\")\n",
                "s1_normal = create_prefixes_stage1(normal_train, is_failure=False)\n",
                "s1_failure = create_prefixes_stage1(failure_train, is_failure=True)\n",
                "s1_pos = sum(1 for p in s1_failure if p[1]==1)\n",
                "print(f\"Stage 1: normal={len(s1_normal):,}, failure={len(s1_failure):,}, pos={s1_pos:,}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# STAGE 2: TRUE PRE-WARNING\n",
                "print(\"\\n\" + \"=\"*50)\n",
                "print(\"STAGE 2: TRUE Pre-Warning\")\n",
                "print(\"=\"*50)\n",
                "\n",
                "model.load_state_dict(torch.load(f\"{OUTPUT_DIR}/hdfs_ew_stage1.pt\", map_location=device))\n",
                "print(\"Loaded Stage 1 checkpoint\")\n",
                "\n",
                "s2_ds = EWDataset(s2_normal, s2_failure, VOCAB_SIZE, ratio=5)\n",
                "s2_loader = DataLoader(s2_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
                "POS_WEIGHT_S2 = len(s2_normal) / max(s2_pos, 1)\n",
                "ew_crit_s2 = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([POS_WEIGHT_S2], device=device))\n",
                "\n",
                "opt = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
                "best_s2 = float('inf')\n",
                "\n",
                "for ep in range(5):\n",
                "    model.train(); tl = 0\n",
                "    for b in tqdm(s2_loader, desc=f\"S2.{ep+1}\"):\n",
                "        ids, mask, ew_l, mlm_l = b['ids'].to(device), b['mask'].to(device), b['ew'].to(device), b['mlm'].to(device)\n",
                "        opt.zero_grad()\n",
                "        if scaler:\n",
                "            with torch.amp.autocast('cuda'):\n",
                "                mlm_lg, cls, ew = model(ids, mask)\n",
                "                loss = 0.2*F.cross_entropy(mlm_lg.view(-1,VOCAB_SIZE), mlm_l.view(-1), ignore_index=-100) + 0.1*torch.mean((cls-model.ctr)**2) + 1.0*ew_crit_s2(ew, ew_l)\n",
                "            scaler.scale(loss).backward(); scaler.step(opt); scaler.update()\n",
                "        else:\n",
                "            mlm_lg, cls, ew = model(ids, mask)\n",
                "            loss = 0.2*F.cross_entropy(mlm_lg.view(-1,VOCAB_SIZE), mlm_l.view(-1), ignore_index=-100) + 0.1*torch.mean((cls-model.ctr)**2) + 1.0*ew_crit_s2(ew, ew_l)\n",
                "            loss.backward(); opt.step()\n",
                "        model.upd(cls.detach()); tl += loss.item()\n",
                "    avg = tl/len(s2_loader); print(f\"  {ep+1}: {avg:.4f}\")\n",
                "    if avg < best_s2: best_s2 = avg; torch.save(model.state_dict(), f\"{OUTPUT_DIR}/hdfs_ew_prewarn.pt\")\n",
                "\n",
                "print(f\"Stage 2 best: {best_s2:.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# EVALUATION (F1-optimal, per-failure)\n",
                "print(\"\\nEVALUATION\")\n",
                "model.load_state_dict(torch.load(f\"{OUTPUT_DIR}/hdfs_ew_prewarn.pt\", map_location=device))\n",
                "model.eval()\n",
                "\n",
                "@torch.no_grad()\n",
                "def score_trace(trace, step=3):\n",
                "    if len(trace) < 6: return []\n",
                "    probs = []\n",
                "    for k in range(6, len(trace)+1, step):\n",
                "        s = [min(t+OFF, VOCAB_SIZE-1) for t in trace[:k][:CONTEXT_LEN-2]]\n",
                "        tok = [CLS]+s+[SEP]+[PAD]*(CONTEXT_LEN-len(s)-2)\n",
                "        ids = torch.tensor([tok], device=device)\n",
                "        _, _, ew = model(ids)\n",
                "        probs.append((k, torch.sigmoid(ew).item()))\n",
                "    return probs\n",
                "\n",
                "failure_scores = []\n",
                "for t in tqdm(failure_test, desc=\"F\"):\n",
                "    probs = score_trace(t)\n",
                "    if probs: failure_scores.append({'trace': t, 'max_prob': max(p for _,p in probs), 'probs': probs})\n",
                "\n",
                "normal_scores = []\n",
                "for t in tqdm(normal_test, desc=\"N\"):\n",
                "    probs = score_trace(t)\n",
                "    if probs: normal_scores.append({'trace': t, 'max_prob': max(p for _,p in probs)})\n",
                "\n",
                "print(f\"Scored: {len(failure_scores)} F, {len(normal_scores)} N\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# F1-OPTIMAL + METRICS\n",
                "all_probs = [s['max_prob'] for s in failure_scores] + [s['max_prob'] for s in normal_scores]\n",
                "all_labels = [1]*len(failure_scores) + [0]*len(normal_scores)\n",
                "\n",
                "auc = roc_auc_score(all_labels, all_probs) if len(set(all_labels)) > 1 else 0.5\n",
                "prec, rec, thr = precision_recall_curve(all_labels, all_probs)\n",
                "f1 = 2*prec*rec/(prec+rec+1e-9)\n",
                "best_idx = f1.argmax()\n",
                "TH = thr[best_idx] if best_idx < len(thr) else thr[-1] if len(thr) > 0 else 0.5\n",
                "\n",
                "tp, leads = [], []\n",
                "for s in failure_scores:\n",
                "    if s['max_prob'] > TH:\n",
                "        tp.append(s)\n",
                "        for k, p in s['probs']:\n",
                "            if p > TH: leads.append(len(s['trace']) - k); break\n",
                "fp = [s for s in normal_scores if s['max_prob'] > TH]\n",
                "\n",
                "recall = len(tp)/len(failure_scores) if failure_scores else 0\n",
                "far = len(fp)/len(normal_scores) if normal_scores else 0\n",
                "precision = len(tp)/(len(tp)+len(fp)) if (len(tp)+len(fp))>0 else 0\n",
                "f1_final = 2*precision*recall/(precision+recall) if (precision+recall)>0 else 0\n",
                "\n",
                "print(f\"\\n=== RESULTS ===\")\n",
                "print(f\"AUC: {auc:.4f}, TH: {TH:.4f}\")\n",
                "print(f\"F1: {f1_final:.4f}, P: {precision:.4f}, R: {recall:.4f}, FAR: {far:.4f}\")\n",
                "if leads: print(f\"Lead: median={np.median(leads):.0f}, mean={np.mean(leads):.0f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# SAVE\n",
                "results = {'dataset': 'HDFS', 'method': 'curriculum_true_prewarn',\n",
                "    'metrics': {'auc': float(auc), 'threshold': float(TH), 'f1': float(f1_final),\n",
                "        'precision': float(precision), 'recall': float(recall), 'far': float(far),\n",
                "        'lead_median': float(np.median(leads)) if leads else 0,\n",
                "        'lead_mean': float(np.mean(leads)) if leads else 0}}\n",
                "with open(f\"{OUTPUT_DIR}/hdfs_ew_results.json\", 'w') as f: json.dump(results, f, indent=2)\n",
                "print(f\"Saved: {OUTPUT_DIR}/hdfs_ew_results.json\")\n",
                "print(json.dumps(results['metrics'], indent=2))"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}