{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# BGL Early Warning (Paper-style + Phase 3)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os, json, random\n",
                "import numpy as np\n",
                "from tqdm import tqdm\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "from sklearn.metrics import roc_auc_score, precision_recall_curve\n",
                "\n",
                "!pip install drain3 -q\n",
                "from drain3 import TemplateMiner\n",
                "from drain3.template_miner_config import TemplateMinerConfig\n",
                "\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f\"Device: {device}\")\n",
                "\n",
                "# ===== KAGGLE PATHS =====\n",
                "BASE = \"/kaggle/working\"\n",
                "BGL_FILE = \"/kaggle/input/loghub-bgl-log-data/BGL.log\"\n",
                "CKPT_DIR = \"/kaggle/input/bgltest\"\n",
                "EW_OUTPUT = f\"{BASE}/output-bglew3\"\n",
                "os.makedirs(EW_OUTPUT, exist_ok=True)\n",
                "\n",
                "CONTEXT_LEN, D_MODEL, N_HEADS, N_LAYERS = 128, 256, 8, 4\n",
                "PAD, CLS, MASK, SEP = 0, 1, 2, 3\n",
                "BATCH_SIZE, SEED = 64, 42\n",
                "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
                "\n",
                "DELTA_MIN = 5\n",
                "DELTA_SEC = DELTA_MIN * 60\n",
                "WINDOW_SIZE, STEP_SIZE = 128, 8\n",
                "SAMPLING_RATIO = 5"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ===== LOAD PARSED DATA (FROM LOCAL CPU) =====\n",
                "import pickle\n",
                "\n",
                "# Kaggle path - update if needed\n",
                "PKL_FILE = \"/kaggle/input/bgl-parsed/bgl_parsed_data.pkl\"\n",
                "\n",
                "with open(PKL_FILE, 'rb') as f:\n",
                "    data = pickle.load(f)\n",
                "\n",
                "bgl_events = data['bgl_events']\n",
                "normal_traces = data['normal_traces']\n",
                "failure_traces = data['failure_traces']\n",
                "train_normal = data['train_normal']\n",
                "test_normal = data['test_normal']\n",
                "train_failure = data['train_failure']\n",
                "test_failure = data['test_failure']\n",
                "train_windows = data['train_windows']\n",
                "NUM_CLUSTERS = data['NUM_CLUSTERS']\n",
                "VOCAB_SIZE = data['VOCAB_SIZE']\n",
                "OLD_VOCAB_SIZE = data['OLD_VOCAB_SIZE']\n",
                "\n",
                "print(f\"Loaded parsed data!\")\n",
                "print(f\"Events: {len(bgl_events):,}\")\n",
                "print(f\"VOCAB_SIZE: {VOCAB_SIZE}\")\n",
                "print(f\"Train windows: {len(train_windows):,}\")\n",
                "print(f\"Test: {len(test_normal)} N, {len(test_failure)} F\")\n",
                "print(\"\\n>>> SKIP cells: LOAD CHECKPOINT, PARSE BGL, CREATE TRACES, CREATE WINDOWS <<<\")\n",
                "print(\">>> GO DIRECTLY TO: DATASET & MODEL <<<\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# LOAD CHECKPOINT\n",
                "ckpt = torch.load(f\"{CKPT_DIR}/checkpoints_logbert_ep11.pt\", map_location=device)\n",
                "OLD_VOCAB_SIZE = ckpt['model_state_dict']['tok.weight'].shape[0]\n",
                "print(f\"HDFS VOCAB: {OLD_VOCAB_SIZE}\")\n",
                "\n",
                "config = TemplateMinerConfig()\n",
                "config.drain_depth = 4\n",
                "config.drain_sim_th = 0.4\n",
                "miner = TemplateMiner(config=config)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# PARSE BGL\n",
                "print(\"PARSING BGL\")\n",
                "\n",
                "def get_cluster_id(result):\n",
                "    return result.get('cluster_id', 0) if isinstance(result, dict) else result.cluster_id\n",
                "\n",
                "bgl_events = []\n",
                "with open(BGL_FILE, 'r', errors='ignore') as f:\n",
                "    for line in tqdm(f, desc=\"BGL\"):\n",
                "        line = line.strip()\n",
                "        if not line: continue\n",
                "        parts = line.split(None, 9)\n",
                "        if len(parts) < 2: continue\n",
                "        is_anomaly = parts[0] != '-'\n",
                "        try: timestamp = int(parts[1])\n",
                "        except: timestamp = None\n",
                "        message = parts[-1]\n",
                "        tid = OLD_VOCAB_SIZE + get_cluster_id(miner.add_log_message(message))\n",
                "        bgl_events.append((timestamp, tid, is_anomaly))\n",
                "\n",
                "NUM_CLUSTERS = len(miner.drain.clusters)\n",
                "VOCAB_SIZE = OLD_VOCAB_SIZE + NUM_CLUSTERS + 10\n",
                "print(f\"Events: {len(bgl_events):,}, BGL Clusters: {NUM_CLUSTERS}, Total VOCAB: {VOCAB_SIZE}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# CREATE TRACES\n",
                "print(\"\\nCREATING TRACES\")\n",
                "SESSION_GAP = 3600\n",
                "sessions, current, last_time = [], [], None\n",
                "\n",
                "for ts, tid, anom in bgl_events:\n",
                "    if last_time and ts and ts - last_time > SESSION_GAP:\n",
                "        if current: sessions.append(current)\n",
                "        current = []\n",
                "    current.append((ts, tid, anom))\n",
                "    last_time = ts if ts else last_time\n",
                "if current: sessions.append(current)\n",
                "\n",
                "normal_traces, failure_traces = [], []\n",
                "for sess in sessions:\n",
                "    tids = [tid for _,tid,_ in sess]\n",
                "    if len(tids) >= 20:\n",
                "        if any(a for _,_,a in sess):\n",
                "            failure_traces.append({'tids': tids, 'events': sess})\n",
                "        else:\n",
                "            normal_traces.append({'tids': tids, 'events': sess})\n",
                "\n",
                "print(f\"Normal: {len(normal_traces)}, Failure: {len(failure_traces)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# CREATE WINDOWS\n",
                "WINDOW_SIZE, STEP_SIZE = 128, 8\n",
                "\n",
                "def create_windows(traces, is_failure=False):\n",
                "    windows = []\n",
                "    for trace in traces:\n",
                "        tids, events = trace['tids'], trace.get('events')\n",
                "        n = len(tids)\n",
                "        for i in range(0, max(1, n - WINDOW_SIZE + 1), STEP_SIZE):\n",
                "            end = min(i + WINDOW_SIZE, n)\n",
                "            w_tids = tids[i:end]\n",
                "            if is_failure and events:\n",
                "                end_time = events[min(end-1, n-1)][0]\n",
                "                has_future = False\n",
                "                if end_time:\n",
                "                    for j in range(end, n):\n",
                "                        if events[j][0] and events[j][0] > end_time + DELTA_SEC: break\n",
                "                        if events[j][2]: has_future = True; break\n",
                "                has_current = any(events[k][2] for k in range(i, min(end, n)))\n",
                "                label = 1 if (has_current or has_future) else 0\n",
                "            else:\n",
                "                label = 0\n",
                "            windows.append({'tids': w_tids, 'label': label})\n",
                "    return windows\n",
                "\n",
                "rng = np.random.RandomState(SEED)\n",
                "n_idx, f_idx = rng.permutation(len(normal_traces)), rng.permutation(len(failure_traces))\n",
                "n_split, f_split = int(0.8*len(normal_traces)), int(0.8*len(failure_traces))\n",
                "train_normal = [normal_traces[i] for i in n_idx[:n_split]]\n",
                "test_normal = [normal_traces[i] for i in n_idx[n_split:]]\n",
                "train_failure = [failure_traces[i] for i in f_idx[:f_split]]\n",
                "test_failure = [failure_traces[i] for i in f_idx[f_split:]]\n",
                "\n",
                "train_windows = create_windows(train_normal) + create_windows(train_failure, True)\n",
                "n_pos = sum(1 for w in train_windows if w['label']==1)\n",
                "print(f\"Train: {len(train_windows):,} (pos={n_pos:,}), Test: {len(test_normal)} N, {len(test_failure)} F\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# SAVE PARSED DATA (for re-evaluation without re-parsing)\n",
                "import pickle\n",
                "save_data = {\n",
                "    'bgl_events': bgl_events,\n",
                "    'normal_traces': normal_traces,\n",
                "    'failure_traces': failure_traces,\n",
                "    'NUM_CLUSTERS': NUM_CLUSTERS,\n",
                "    'VOCAB_SIZE': VOCAB_SIZE,\n",
                "    'OLD_VOCAB_SIZE': OLD_VOCAB_SIZE,\n",
                "    'train_normal': train_normal,\n",
                "    'test_normal': test_normal,\n",
                "    'train_failure': train_failure,\n",
                "    'test_failure': test_failure\n",
                "}\n",
                "with open(f\"{EW_OUTPUT}/bgl_parsed_data.pkl\", 'wb') as f:\n",
                "    pickle.dump(save_data, f)\n",
                "print(f\"Saved: {EW_OUTPUT}/bgl_parsed_data.pkl\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# DATASET & MODEL\n",
                "class BalancedDataset(Dataset):\n",
                "    def __init__(self, windows, vs, ratio=SAMPLING_RATIO):\n",
                "        self.pos = [w for w in windows if w['label']==1]\n",
                "        self.neg = [w for w in windows if w['label']==0]\n",
                "        self.vs, self.ratio = vs, ratio\n",
                "        self.n = len(self.pos)*(1+ratio) if self.pos else len(self.neg)\n",
                "    def __len__(self): return self.n\n",
                "    def __getitem__(self, idx):\n",
                "        w = random.choice(self.pos) if (self.pos and idx%(self.ratio+1)==0) else random.choice(self.neg if self.neg else self.pos)\n",
                "        s = [min(t, self.vs-1) for t in w['tids'][:CONTEXT_LEN-2]]\n",
                "        tok = [CLS]+s+[SEP]+[PAD]*(CONTEXT_LEN-len(s)-2)\n",
                "        masked, mlm_l = tok.copy(), [-100]*len(tok)\n",
                "        if len(s)>0:\n",
                "            for p in np.random.choice(len(s), min(max(1,int(len(s)*0.15)), len(s)), replace=False)+1:\n",
                "                mlm_l[p] = masked[p]; masked[p] = MASK\n",
                "        return {'ids': torch.tensor(masked), 'mask': torch.tensor([1 if t!=PAD else 0 for t in masked]),\n",
                "                'ew_label': torch.tensor(w['label'], dtype=torch.float), 'mlm_labels': torch.tensor(mlm_l)}\n",
                "\n",
                "class LogBERTEW(nn.Module):\n",
                "    def __init__(self, vs):\n",
                "        super().__init__()\n",
                "        self.tok = nn.Embedding(vs, D_MODEL, padding_idx=PAD)\n",
                "        self.pos = nn.Embedding(CONTEXT_LEN, D_MODEL)\n",
                "        self.drop = nn.Dropout(0.1)\n",
                "        self.enc = nn.TransformerEncoder(nn.TransformerEncoderLayer(D_MODEL, N_HEADS, D_MODEL*4, 0.1, 'gelu', batch_first=True), N_LAYERS)\n",
                "        self.head = nn.Linear(D_MODEL, vs)\n",
                "        self.ew_head = nn.Linear(D_MODEL, 1)\n",
                "        self.register_buffer('ctr', torch.zeros(D_MODEL)); self.ci = False\n",
                "    def forward(self, ids, mask=None):\n",
                "        x = self.tok(ids) + self.pos(torch.arange(ids.size(1), device=ids.device))\n",
                "        h = self.enc(self.drop(x), src_key_padding_mask=(mask==0) if mask is not None else None)\n",
                "        return self.head(h), h[:,0,:], self.ew_head(h[:,0,:]).squeeze(-1)\n",
                "    def upd(self, e):\n",
                "        with torch.no_grad(): bc = e.mean(0); self.ctr = bc if not self.ci else 0.9*self.ctr+0.1*bc; self.ci = True"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# MODEL INIT\n",
                "model = LogBERTEW(VOCAB_SIZE).to(device)\n",
                "with torch.no_grad():\n",
                "    model.tok.weight[:OLD_VOCAB_SIZE] = ckpt['model_state_dict']['tok.weight']\n",
                "    nn.init.normal_(model.tok.weight[OLD_VOCAB_SIZE:], std=0.02)\n",
                "state = {k:v for k,v in ckpt['model_state_dict'].items() if 'tok.weight' not in k and 'head.' not in k}\n",
                "model.load_state_dict(state, strict=False)\n",
                "with torch.no_grad():\n",
                "    model.head.weight[:OLD_VOCAB_SIZE] = ckpt['model_state_dict']['head.weight']\n",
                "    model.head.bias[:OLD_VOCAB_SIZE] = ckpt['model_state_dict']['head.bias']\n",
                "model.ctr = ckpt['center'].to(device); model.ci = True\n",
                "print(f\"Model loaded: {OLD_VOCAB_SIZE} -> {VOCAB_SIZE}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# TRAINING\n",
                "LAMBDA_MLM, LAMBDA_VHM, MU_EW = 0.4, 0.1, 1.0\n",
                "train_pos = sum(1 for w in train_windows if w['label']==1)\n",
                "POS_WEIGHT = (len(train_windows)-train_pos) / max(train_pos, 1)\n",
                "train_ds = BalancedDataset(train_windows, VOCAB_SIZE)\n",
                "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
                "ew_crit = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([POS_WEIGHT], device=device))\n",
                "scaler = torch.amp.GradScaler('cuda') if device.type=='cuda' else None\n",
                "best = float('inf')\n",
                "\n",
                "# Phase 1\n",
                "print(\"PHASE 1\")\n",
                "for p in model.parameters(): p.requires_grad = False\n",
                "for p in model.ew_head.parameters(): p.requires_grad = True\n",
                "opt = torch.optim.Adam(model.ew_head.parameters(), lr=1e-4)\n",
                "for ep in range(2):\n",
                "    model.train(); tl = 0\n",
                "    for b in tqdm(train_loader, desc=f\"P1.{ep+1}\"):\n",
                "        opt.zero_grad(); _, _, ew = model(b['ids'].to(device), b['mask'].to(device))\n",
                "        loss = ew_crit(ew, b['ew_label'].to(device)); loss.backward(); opt.step(); tl += loss.item()\n",
                "    print(f\"  {ep+1}: {tl/len(train_loader):.4f}\")\n",
                "\n",
                "# Phase 2\n",
                "print(\"\\nPHASE 2\")\n",
                "for p in model.parameters(): p.requires_grad = True\n",
                "opt = torch.optim.AdamW(model.parameters(), lr=3e-5)\n",
                "for ep in range(8):\n",
                "    model.train(); tl = 0\n",
                "    for b in tqdm(train_loader, desc=f\"P2.{ep+1}\"):\n",
                "        ids, mask, ew_l, mlm_l = b['ids'].to(device), b['mask'].to(device), b['ew_label'].to(device), b['mlm_labels'].to(device)\n",
                "        opt.zero_grad()\n",
                "        if scaler:\n",
                "            with torch.amp.autocast('cuda'):\n",
                "                mlm_lg, cls, ew = model(ids, mask)\n",
                "                loss = LAMBDA_MLM*F.cross_entropy(mlm_lg.view(-1,VOCAB_SIZE), mlm_l.view(-1), ignore_index=-100) + LAMBDA_VHM*torch.mean((cls-model.ctr)**2) + MU_EW*ew_crit(ew, ew_l)\n",
                "            scaler.scale(loss).backward(); scaler.step(opt); scaler.update()\n",
                "        else:\n",
                "            mlm_lg, cls, ew = model(ids, mask)\n",
                "            loss = LAMBDA_MLM*F.cross_entropy(mlm_lg.view(-1,VOCAB_SIZE), mlm_l.view(-1), ignore_index=-100) + LAMBDA_VHM*torch.mean((cls-model.ctr)**2) + MU_EW*ew_crit(ew, ew_l)\n",
                "            loss.backward(); opt.step()\n",
                "        model.upd(cls.detach()); tl += loss.item()\n",
                "    avg = tl/len(train_loader); print(f\"  {ep+1}: {avg:.4f}\")\n",
                "    if avg < best: best = avg; torch.save(model.state_dict(), f\"{EW_OUTPUT}/bgl_ew_best.pt\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# PHASE 3: Fine-tune LR=1e-5 + early stop\n",
                "print(\"\\nPHASE 3 (LR=1e-5 + early-stop)\")\n",
                "opt = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
                "patience, no_improve = 2, 0\n",
                "\n",
                "for ep in range(8):\n",
                "    model.train(); tl = 0\n",
                "    for b in tqdm(train_loader, desc=f\"P3.{ep+1}\"):\n",
                "        ids, mask, ew_l, mlm_l = b['ids'].to(device), b['mask'].to(device), b['ew_label'].to(device), b['mlm_labels'].to(device)\n",
                "        opt.zero_grad()\n",
                "        if scaler:\n",
                "            with torch.amp.autocast('cuda'):\n",
                "                mlm_lg, cls, ew = model(ids, mask)\n",
                "                loss = LAMBDA_MLM*F.cross_entropy(mlm_lg.view(-1,VOCAB_SIZE), mlm_l.view(-1), ignore_index=-100) + LAMBDA_VHM*torch.mean((cls-model.ctr)**2) + MU_EW*ew_crit(ew, ew_l)\n",
                "            scaler.scale(loss).backward(); scaler.step(opt); scaler.update()\n",
                "        else:\n",
                "            mlm_lg, cls, ew = model(ids, mask)\n",
                "            loss = LAMBDA_MLM*F.cross_entropy(mlm_lg.view(-1,VOCAB_SIZE), mlm_l.view(-1), ignore_index=-100) + LAMBDA_VHM*torch.mean((cls-model.ctr)**2) + MU_EW*ew_crit(ew, ew_l)\n",
                "            loss.backward(); opt.step()\n",
                "        model.upd(cls.detach()); tl += loss.item()\n",
                "    avg = tl/len(train_loader); print(f\"  {ep+1}: {avg:.4f}\")\n",
                "    if avg < best:\n",
                "        best = avg; torch.save(model.state_dict(), f\"{EW_OUTPUT}/bgl_ew_best.pt\"); no_improve = 0\n",
                "    else:\n",
                "        no_improve += 1\n",
                "        if no_improve >= patience: print(f\"  Early stop\"); break\n",
                "print(f\"Best loss: {best:.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# EVALUATION\n",
                "print(\"\\nEVALUATION\")\n",
                "model.eval()\n",
                "\n",
                "@torch.no_grad()\n",
                "def score_trace(tids, step=8):\n",
                "    probs = []\n",
                "    for k in range(min(WINDOW_SIZE, len(tids)), len(tids)+1, step):\n",
                "        s = [min(t, VOCAB_SIZE-1) for t in tids[max(0,k-WINDOW_SIZE):k]]\n",
                "        if len(s) < 10: continue\n",
                "        tok = [CLS]+s[:CONTEXT_LEN-2]+[SEP]+[PAD]*(CONTEXT_LEN-len(s[:CONTEXT_LEN-2])-2)\n",
                "        ids = torch.tensor([tok], device=device)\n",
                "        _, _, ew = model(ids)\n",
                "        probs.append((k, torch.sigmoid(ew).item()))\n",
                "    return probs\n",
                "\n",
                "failure_scores = [{'trace': t, 'max_prob': max(p for _,p in score_trace(t['tids'])), 'probs': score_trace(t['tids'])} for t in tqdm(test_failure) if score_trace(t['tids'])]\n",
                "normal_scores = [{'trace': t, 'max_prob': max(p for _,p in score_trace(t['tids']))} for t in tqdm(test_normal) if score_trace(t['tids'])]\n",
                "print(f\"Scored: {len(failure_scores)} F, {len(normal_scores)} N\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# F1-OPTIMAL + METRICS (FIXED: lead to first anomaly)\n",
                "all_probs = [s['max_prob'] for s in failure_scores] + [s['max_prob'] for s in normal_scores]\n",
                "all_labels = [1]*len(failure_scores) + [0]*len(normal_scores)\n",
                "\n",
                "auc = roc_auc_score(all_labels, all_probs) if len(set(all_labels)) > 1 else 0.5\n",
                "prec, rec, thr = precision_recall_curve(all_labels, all_probs)\n",
                "f1 = 2*prec*rec/(prec+rec+1e-9)\n",
                "best_idx = f1.argmax()\n",
                "TH = thr[best_idx] if best_idx < len(thr) else thr[-1] if len(thr) > 0 else 0.5\n",
                "\n",
                "tp, leads = [], []\n",
                "for s in failure_scores:\n",
                "    if s['max_prob'] > TH:\n",
                "        tp.append(s)\n",
                "        # FIXED: Lead to FIRST ANOMALY, not trace end\n",
                "        events = s['trace'].get('events', [])\n",
                "        first_anom_idx = next((i for i, e in enumerate(events) if e[2]), len(events))\n",
                "        for k, p in s['probs']:\n",
                "            if p > TH:\n",
                "                lead = first_anom_idx - k\n",
                "                if lead > 0:\n",
                "                    leads.append(lead)\n",
                "                break\n",
                "fp = [s for s in normal_scores if s['max_prob'] > TH]\n",
                "\n",
                "recall = len(tp)/len(failure_scores) if failure_scores else 0\n",
                "far = len(fp)/len(normal_scores) if normal_scores else 0\n",
                "precision = len(tp)/(len(tp)+len(fp)) if (len(tp)+len(fp))>0 else 0\n",
                "f1_final = 2*precision*recall/(precision+recall) if (precision+recall)>0 else 0\n",
                "\n",
                "print(f\"\\n=== RESULTS ===\")\n",
                "print(f\"AUC: {auc:.4f}, TH: {TH:.4f}\")\n",
                "print(f\"F1: {f1_final:.4f}, P: {precision:.4f}, R: {recall:.4f}, FAR: {far:.4f}\")\n",
                "if leads: print(f\"Lead: median={np.median(leads):.0f}, mean={np.mean(leads):.0f} (to first anomaly)\")\n",
                "print(f\"TP: {len(tp)}, FP: {len(fp)}, FN: {len(failure_scores)-len(tp)}, TN: {len(normal_scores)-len(fp)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# SAVE\n",
                "results = {'dataset': 'BGL', 'method': 'joint_ew_head_v2',\n",
                "    'metrics': {'auc': float(auc), 'threshold': float(TH), 'f1': float(f1_final),\n",
                "        'precision': float(precision), 'recall': float(recall), 'far': float(far),\n",
                "        'lead_median': float(np.median(leads)) if leads else 0,\n",
                "        'lead_mean': float(np.mean(leads)) if leads else 0}}\n",
                "with open(f\"{EW_OUTPUT}/bgl_ew_results.json\", 'w') as f: json.dump(results, f, indent=2)\n",
                "print(f\"Saved: {EW_OUTPUT}/bgl_ew_results.json\")\n",
                "print(json.dumps(results['metrics'], indent=2))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Post-Training Analysis: validation-calibrated thresholds, lead-to-first-anomaly, FAR\u2013Recall, PR/ROC, CIs\n",
                "import math, statistics\n",
                "from sklearn.metrics import auc, confusion_matrix\n",
                "\n",
                "# 1) Helpers --------------------------------------------------------------\n",
                "\n",
                "def first_anomaly_index(trace):\n",
                "    events = trace.get('events') or []\n",
                "    for i, (_, _, is_anom) in enumerate(events):\n",
                "        if is_anom:\n",
                "            return i\n",
                "    return None\n",
                "\n",
                "@torch.no_grad()\n",
                "def trace_scores_pre_anom(trace, step=8):\n",
                "    tids = trace['tids']\n",
                "    probs = score_trace(tids, step=step)\n",
                "    q = first_anomaly_index(trace)\n",
                "    if q is None:\n",
                "        max_pre = max((p for _, p in probs), default=0.0)\n",
                "    else:\n",
                "        max_pre = max((p for k, p in probs if (k - 1) < q), default=0.0)\n",
                "    return max_pre, probs, q\n",
                "\n",
                "\n",
                "def earliest_crossing_k(probs, threshold, first_idx):\n",
                "    for k, p in probs:\n",
                "        if p > threshold and (first_idx is None or (k - 1) < first_idx):\n",
                "            return k\n",
                "    return None\n",
                "\n",
                "\n",
                "def lead_events_from_crossing(k_cross, first_idx):\n",
                "    if k_cross is None or first_idx is None:\n",
                "        return None\n",
                "    return max(0, first_idx - k_cross + 1)\n",
                "\n",
                "\n",
                "def split_validation_test(norm_list, fail_list, val_ratio=0.3, seed=SEED):\n",
                "    rng = np.random.RandomState(seed)\n",
                "    nN, nF = len(norm_list), len(fail_list)\n",
                "    nN_val = int(max(1, round(nN * val_ratio))) if nN > 0 else 0\n",
                "    nF_val = int(max(1, round(nF * val_ratio))) if nF > 0 else 0\n",
                "    n_idx = rng.permutation(nN) if nN > 0 else np.array([], dtype=int)\n",
                "    f_idx = rng.permutation(nF) if nF > 0 else np.array([], dtype=int)\n",
                "    val_norm = [norm_list[i] for i in n_idx[:nN_val]]\n",
                "    tst_norm = [norm_list[i] for i in n_idx[nN_val:]]\n",
                "    val_fail = [fail_list[i] for i in f_idx[:nF_val]]\n",
                "    tst_fail = [fail_list[i] for i in f_idx[nF_val:]]\n",
                "    return val_norm, val_fail, tst_norm, tst_fail\n",
                "\n",
                "# 2) Build validation/test splits ----------------------------------------\n",
                "val_normal, val_failure, final_normal, final_failure = split_validation_test(\n",
                "    test_normal, test_failure, val_ratio=0.3, seed=SEED)\n",
                "print(f\"Validation: {len(val_normal)} N, {len(val_failure)} F | Test: {len(final_normal)} N, {len(final_failure)} F\")\n",
                "\n",
                "val_norm_scores = [trace_scores_pre_anom(t)[0] for t in tqdm(val_normal, desc=\"VAL normal scores\")]\n",
                "val_fail_scores = [trace_scores_pre_anom(t)[0] for t in tqdm(val_failure, desc=\"VAL failure scores\")]\n",
                "\n",
                "final_norm_sc = []\n",
                "final_fail_sc = []\n",
                "final_fail_meta = []\n",
                "for t in tqdm(final_normal, desc=\"TEST normal scores\"):\n",
                "    s, _, _ = trace_scores_pre_anom(t)\n",
                "    final_norm_sc.append(s)\n",
                "for t in tqdm(final_failure, desc=\"TEST failure scores\"):\n",
                "    s, pr, q = trace_scores_pre_anom(t)\n",
                "    final_fail_sc.append(s)\n",
                "    final_fail_meta.append((pr, q))\n",
                "\n",
                "# 3) Threshold by FAR on validation --------------------------------------\n",
                "far_targets = [0.001, 0.005, 0.01, 0.02, 0.05]\n",
                "\n",
                "def threshold_for_far(norm_scores, far):\n",
                "    if len(norm_scores) == 0:\n",
                "        return 1.0\n",
                "    q = max(0.0, min(1.0, 1.0 - far))\n",
                "    return float(np.quantile(norm_scores, q))\n",
                "\n",
                "calib = {float(ft): threshold_for_far(val_norm_scores, ft) for ft in far_targets}\n",
                "print(\"Calibrated thresholds (VAL):\", {k: round(v, 4) for k, v in calib.items()})\n",
                "\n",
                "# 4) Evaluate on TEST -----------------------------------------------------\n",
                "\n",
                "def evaluate_at_threshold(th):\n",
                "    tp = 0; fp = 0; leads = []\n",
                "    for (probs, q) in final_fail_meta:\n",
                "        kx = earliest_crossing_k(probs, th, q)\n",
                "        if kx is not None:\n",
                "            tp += 1\n",
                "            ld = lead_events_from_crossing(kx, q)\n",
                "            if ld is not None: leads.append(ld)\n",
                "    for s in final_norm_sc:\n",
                "        if s > th: fp += 1\n",
                "    nF = len(final_fail_meta) or 1\n",
                "    nN = len(final_norm_sc) or 1\n",
                "    recall = tp / nF\n",
                "    far = fp / nN\n",
                "    precision = tp / max(tp + fp, 1)\n",
                "    f1 = 2 * precision * recall / max(precision + recall, 1e-9)\n",
                "    lead_stats = {\n",
                "        'median': float(np.median(leads)) if leads else 0.0,\n",
                "        'mean': float(np.mean(leads)) if leads else 0.0,\n",
                "        'p25': float(np.percentile(leads, 25)) if leads else 0.0,\n",
                "        'p75': float(np.percentile(leads, 75)) if leads else 0.0,\n",
                "        'count': len(leads)\n",
                "    }\n",
                "    return {'f1': float(f1), 'precision': float(precision), 'recall': float(recall),\n",
                "            'far': float(far), 'lead': lead_stats,\n",
                "            'tp': int(tp), 'fp': int(fp), 'nF': int(nF), 'nN': int(nN)}\n",
                "\n",
                "far_curve = []\n",
                "for ft in far_targets:\n",
                "    th = calib[float(ft)]\n",
                "    m = evaluate_at_threshold(th)\n",
                "    far_curve.append({'far_target': float(ft), 'threshold': th, **m})\n",
                "\n",
                "op_target = 0.01 if 0.01 in far_targets else far_targets[min(2, len(far_targets)-1)]\n",
                "op_th = calib[float(op_target)]\n",
                "op_metrics = evaluate_at_threshold(op_th)\n",
                "\n",
                "# 5) PR/ROC on TEST -------------------------------------------------------\n",
                "all_scores = np.array(final_fail_sc + final_norm_sc, dtype=float)\n",
                "all_labels = np.array([1]*len(final_fail_sc) + [0]*len(final_norm_sc), dtype=int)\n",
                "roc = float(roc_auc_score(all_labels, all_scores)) if len(set(all_labels))>1 else 0.5\n",
                "pr_p, pr_r, _ = precision_recall_curve(all_labels, all_scores)\n",
                "pr = float(auc(pr_r, pr_p)) if len(pr_r)>1 else 0.0\n",
                "\n",
                "# 6) Bootstrap 95% CI -----------------------------------------------------\n",
                "\n",
                "def bootstrap_ci(n_boot=300, seed=SEED):\n",
                "    rng = np.random.RandomState(seed)\n",
                "    f_idx = np.arange(len(final_fail_meta))\n",
                "    n_idx = np.arange(len(final_norm_sc))\n",
                "    f1s, recs, fars = [], [], []\n",
                "    for _ in range(n_boot):\n",
                "        bf = rng.choice(f_idx, size=len(f_idx), replace=True) if len(f_idx)>0 else []\n",
                "        bn = rng.choice(n_idx, size=len(n_idx), replace=True) if len(n_idx)>0 else []\n",
                "        tp = 0; fp = 0\n",
                "        for i in bf:\n",
                "            probs, q = final_fail_meta[i]\n",
                "            if earliest_crossing_k(probs, op_th, q) is not None:\n",
                "                tp += 1\n",
                "        for j in bn:\n",
                "            if final_norm_sc[j] > op_th:\n",
                "                fp += 1\n",
                "        nF = max(1, len(f_idx)); nN = max(1, len(n_idx))\n",
                "        recall = tp / nF\n",
                "        far = fp / nN\n",
                "        precision = tp / max(tp + fp, 1)\n",
                "        f1 = 2*precision*recall / max(precision+recall, 1e-9)\n",
                "        f1s.append(f1); recs.append(recall); fars.append(far)\n",
                "    def ci(arr):\n",
                "        if not arr: return [0.0, 0.0]\n",
                "        lo, hi = np.percentile(arr, [2.5, 97.5])\n",
                "        return [float(lo), float(hi)]\n",
                "    return {'f1': ci(f1s), 'recall': ci(recs), 'far': ci(fars)}\n",
                "\n",
                "cis = bootstrap_ci()\n",
                "\n",
                "# 7) Save consolidated JSON -----------------------------------------------\n",
                "report = {\n",
                "    'dataset': 'BGL',\n",
                "    'method': 'true_prewarn_analysis',\n",
                "    'config': {\n",
                "        'delta_min': int(DELTA_MIN),\n",
                "        'window_size': int(WINDOW_SIZE),\n",
                "        'step_size': 32,\n",
                "        'far_targets': far_targets\n",
                "    },\n",
                "    'validation': {\n",
                "        'n_normal': len(val_normal), 'n_failure': len(val_failure),\n",
                "        'thresholds_by_far': {str(k): float(v) for k, v in calib.items()}\n",
                "    },\n",
                "    'test': {\n",
                "        'n_normal': len(final_normal), 'n_failure': len(final_failure),\n",
                "        'operating_point': {'far_target': float(op_target), 'threshold': float(op_th)},\n",
                "        'metrics': {\n",
                "            'roc_auc': roc,\n",
                "            'pr_auc': pr,\n",
                "            **op_metrics\n",
                "        },\n",
                "        'ci_95': cis\n",
                "    },\n",
                "    'far_recall_curve': far_curve,\n",
                "    'notes': 'Threshold calibrated on validation normals by per-trace FAR; lead measured to FIRST anomaly event.'\n",
                "}\n",
                "\n",
                "with open(os.path.join(EW_OUTPUT, 'bgl_ew_analysis.json'), 'w') as f:\n",
                "    json.dump(report, f, indent=2)\n",
                "print(f\"\\n\u2713 Saved analysis to {EW_OUTPUT}/bgl_ew_analysis.json\")\n",
                "print(json.dumps(report['test']['metrics'], indent=2))"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}